{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import html_to_json\n",
    "import sys, fitz\n",
    "import spacy\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "from time import time\n",
    "from markdown import markdown as markdown_to_html\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_text(file: str = ''):\n",
    "    reader = PdfReader(file)\n",
    "\n",
    "    isJournal = False\n",
    "    content = ''\n",
    "\n",
    "    doc = fitz.open(file)\n",
    "\n",
    "    for page in doc:\n",
    "\n",
    "        text = page.get_text().encode(\"utf8\")  # get plain text (is in UTF-8)\n",
    "        decoded = text.decode()\n",
    "        \n",
    "        result = re.sub(\n",
    "            r'([\\w\\W])\\s\\n([\\w\\W])|(\\-)\\n([\\w\\W])', '\\g<1> \\g<2>', decoded\n",
    "        , 0, re.MULTILINE)\n",
    "        \n",
    "        hasJournalKeyword = re.search(r'\\s?(Abstract|Abstrak|ABSTRACT|ABSTRAK)\\s?', result)\n",
    "\n",
    "        if hasJournalKeyword:\n",
    "            isJournal = True\n",
    "        \n",
    "        content += result\n",
    "\n",
    "    return ( isJournal, content )\n",
    "\n",
    "def extract(file: str = ''):\n",
    "    \n",
    "    ( journal, content ) = read_text(file)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "    if journal:\n",
    "        \n",
    "        content = re.sub(\n",
    "            r'\\.{2,}', '', re.sub(\n",
    "                r'\\s((m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3}))|(M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))|\\d+)\\.\\s', '\\n\\g<1>. ', re.sub(\n",
    "                    r'\\[(.*)\\]\\s?\\n(.*?)\\n', '[\\\\g<1>](\\\\g<2>)\\n', re.sub(\n",
    "                        r'((m{0,4}(cm|cd|d?c{0,3})(xc|xl|l?x{0,3})(ix|iv|v?i{0,3}))|\\d+)\\)\\n([A-Z])', '\\g<1>) \\g<6>', re.sub( \n",
    "                            r'((M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))|\\d+)\\.\\n([A-Z])', '\\g<1>. \\g<6>', content\n",
    "                        , 0, re.MULTILINE)\n",
    "                    , 0, re.MULTILINE)\n",
    "                , 0, re.MULTILINE)\n",
    "            , 0, re.MULTILINE)\n",
    "        , 0, re.MULTILINE)\n",
    "        \n",
    "        for line in content.split('\\n'):\n",
    "            hasFormula = re.search(r'\\s{2}\\s+', line)\n",
    "\n",
    "            if re.search(r'^([0-9a-zA-Z]|\\[)', line) and len(line) > 7.5 and hasFormula == None:\n",
    "                result += line + '\\n\\n'\n",
    "        \n",
    "        result = re.sub(r'([\\w\\W]{40})\\n{2}([a-z])', '\\g<1> \\g<2>', result)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        result = content\n",
    "    \n",
    "    nlp_entities = []\n",
    "    nlp_result = nlp(result)\n",
    "    \n",
    "    for item in nlp_result.ents:\n",
    "        nlp_entities.append({\n",
    "            'label': item.label_,\n",
    "            'text': item.text\n",
    "        })\n",
    "    \n",
    "    print(result, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " PDF Test File  \n",
      "Congratulations, your computer is equipped with a PDF (Portable Document Format) reader!  You should be able to view any of the PDF documents and forms available on our site.  PDF forms are indicated by these icons:   or  .    \n",
      "Yukon Department of Education Box 2703 Whitehorse,Yukon Canada Y1A 2C6  \n",
      "Please visit our website at:  http://www.education.gov.yk.ca/\n",
      "   \n",
      " (PDF Test File, PDF, Document Format, PDF, PDF, Yukon Department of Education, 2703, Yukon Canada, 2C6)\n"
     ]
    }
   ],
   "source": [
    "extract('./test/pdf/01-simple.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Smallpdf\n",
      "Digital Documents—All In One Place\n",
      "Access Files Anytime, Anywhere Enhance Documents in One Click Collaborate With Others With the new Smallpdf experience, you can freely upload, organize, and share digital documents. When you enable the ‘Storage’ option, we’ll also store all processed files here. You can access files stored on Smallpdf from your computer, phone, or tablet. We’ll also sync files from the Smallpdf Mobile App to our online portal\n",
      "When you right-click on a file, we’ll present you with an array of options to convert, compress, or modify it. Forget mundane administrative tasks. With Smallpdf, you can request e-signatures, send large files, or even enable the Smallpdf G Suite App for your entire organization. Ready to take document management to the next level? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract('./test/pdf/02-text-image.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoice\n",
      "Payment is due within 30 days from date of invoice. Late payment is subject to fees of 5% per month.\n",
      "Thanks for choosing DEMO - Sliced Invoices | admin@slicedinvoices.com\n",
      "Page 1/1\n",
      "From:\n",
      "DEMO - Sliced Invoices\n",
      "Suite 5A-1204\n",
      "123 Somewhere Street\n",
      "Your City AZ 12345\n",
      "admin@slicedinvoices.com\n",
      "Invoice Number\n",
      "INV-3337\n",
      "Order Number\n",
      "12345\n",
      "Invoice Date\n",
      "January 25, 2016\n",
      "Due Date\n",
      "January 31, 2016\n",
      "Total Due\n",
      "$93.50\n",
      "To:\n",
      "Test Business\n",
      "123 Somewhere St\n",
      "Melbourne, VIC 3000\n",
      "test@test.com\n",
      "Hrs/Qty\n",
      "Service\n",
      "Rate/Price\n",
      "Adjust\n",
      "Sub Total\n",
      "1.00\n",
      "Web Design\n",
      "This is a sample description...\n",
      "$85.00\n",
      "0.00%\n",
      "$85.00\n",
      "Sub Total\n",
      "$85.00\n",
      "Tax\n",
      "$8.50\n",
      "Total\n",
      "$93.50\n",
      "ANZ Bank\n",
      "ACC # 1234 1234\n",
      "BSB # 4321 432\n",
      "Paid\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract('./test/pdf/03-invoice.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin: A Peer-to-Peer Electronic Cash System\n",
      "\n",
      "Satoshi Nakamoto satoshin@gmx.com\n",
      "\n",
      "www.bitcoin.org\n",
      "\n",
      "Abstract.  A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution.  Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work.  The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power.  As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers.  The network itself requires minimal structure.  Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.\n",
      "\n",
      "1. Introduction\n",
      "\n",
      "2. Transactions\n",
      "\n",
      "We define an electronic coin as a chain of digital signatures.  Each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner and adding these to the end of the coin.  A payee can verify the signatures to verify the chain of ownership. The problem of course is the payee can't verify that one of the owners did not double-spend the coin.  A common solution is to introduce a trusted central authority, or mint, that checks every transaction for double spending.  After each transaction, the coin must be returned to the mint to issue a new coin, and only coins issued directly from the mint are trusted not to be double-spent. The problem with this solution is that the fate of the entire money system depends on the company running the mint, with every transaction having to go through them, just like a bank. We need a way for the payee to know that the previous owners did not sign any earlier transactions.  For our purposes, the earliest transaction is the one that counts, so we don't care about later attempts to double-spend.  The only way to confirm the absence of a transaction is to be aware of all transactions.  In the mint based model, the mint was aware of all transactions and decided which arrived first.  To accomplish this without a trusted party, transactions must be publicly announced [1], and we need a system for participants to agree on a single history of the order in which they were received.  The payee needs proof that at the time of each transaction, the majority of nodes agreed it was the first received.\n",
      "\n",
      "3. Timestamp Server\n",
      "\n",
      "The solution we propose begins with a timestamp server.  A timestamp server works by taking a hash of a block of items to be timestamped and widely publishing the hash, such as in a newspaper or Usenet post [2-5].  The timestamp proves that the data must have existed at the time, obviously, in order to get into the hash.  Each timestamp includes the previous timestamp in its hash, forming a chain, with each additional timestamp reinforcing the ones before it.\n",
      "\n",
      "Transaction\n",
      "\n",
      "Owner 1's\n",
      "\n",
      "Public Key\n",
      "\n",
      "Owner 0's\n",
      "\n",
      "Signature\n",
      "\n",
      "Transaction\n",
      "\n",
      "Owner 2's\n",
      "\n",
      "Public Key\n",
      "\n",
      "Owner 1's\n",
      "\n",
      "Signature\n",
      "\n",
      "Transaction\n",
      "\n",
      "Owner 3's\n",
      "\n",
      "Public Key\n",
      "\n",
      "Owner 2's\n",
      "\n",
      "Signature\n",
      "\n",
      "Owner 2's\n",
      "\n",
      "Private Key\n",
      "\n",
      "Owner 1's\n",
      "\n",
      "Private Key\n",
      "\n",
      "Sign  Sign  Owner 3's\n",
      "\n",
      "Private Key\n",
      "\n",
      "4. Proof-of-Work\n",
      "\n",
      "To implement a distributed timestamp server on a peer-to-peer basis, we will need to use a proof f-work system similar to Adam Back's Hashcash [6], rather than newspaper or Usenet posts. The proof-of-work involves scanning for a value that when hashed, such as with SHA-256, the hash begins with a number of zero bits.  The average work required is exponential in the number of zero bits required and can be verified by executing a single hash. For our timestamp network, we implement the proof-of-work by incrementing a nonce in the block until a value is found that gives the block's hash the required zero bits.  Once the CPU effort has been expended to make it satisfy the proof-of-work, the block cannot be changed without redoing the work.  As later blocks are chained after it, the work to change the block would include redoing all the blocks after it. The proof-of-work also solves the problem of determining representation in majority decision making.  If the majority were based on one-IP-address-one-vote, it could be subverted by anyone able to allocate many IPs.  Proof-of-work  is essentially one-CPU-one-vote.  The  majority decision is represented by the longest chain, which has the greatest proof-of-work effort invested in it.  If a majority of CPU power is controlled by honest nodes, the honest chain will grow the fastest and outpace any competing chains.  To modify a past block, an attacker would have to redo the proof-of-work of the block and all blocks after it and then catch up with and surpass the work of the honest nodes.  We will show later that the probability of a slower attacker catching up diminishes exponentially as subsequent blocks are added. To compensate for increasing hardware speed and varying interest in running nodes over time, the proof-of-work difficulty is determined by a moving average targeting an average number of blocks per hour.  If they're generated too fast, the difficulty increases.\n",
      "\n",
      "5. Network\n",
      "\n",
      "The steps to run the network are as follows:\n",
      "\n",
      "1) New transactions are broadcast to all nodes.\n",
      "\n",
      "2) Each node collects new transactions into a block.  3) Each node works on finding a difficult proof-of-work for its block.\n",
      "\n",
      "4) When a node finds a proof-of-work, it broadcasts the block to all nodes.\n",
      "\n",
      "5) Nodes accept the block only if all transactions in it are valid and not already spent.\n",
      "\n",
      "6) Nodes express their acceptance of the block by working on creating the next block in the chain, using the hash of the accepted block as the previous hash. Nodes always consider the longest chain to be the correct one and will keep working on extending it.  If two nodes broadcast different versions of the next block simultaneously, some nodes may receive one or the other first.  In that case, they work on the first one they received, but save the other branch in case it becomes longer.  The tie will be broken when the next proof f-work is found and one branch becomes longer; the nodes that were working on the other branch will then switch to the longer one.\n",
      "\n",
      "Prev Hash\n",
      "\n",
      "Prev Hash\n",
      "\n",
      "6. Incentive\n",
      "\n",
      "By convention, the first transaction in a block is a special transaction that starts a new coin owned by the creator of the block.  This adds an incentive for nodes to support the network, and provides a way to initially distribute coins into circulation, since there is no central authority to issue them. The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation.  In our case, it is CPU time and electricity that is expended. The incentive can also be funded with transaction fees.  If the output value of a transaction is less than its input value, the difference is a transaction fee that is added to the incentive value of the block containing the transaction.  Once a predetermined number of coins have entered circulation, the incentive can transition entirely to transaction fees and be completely inflation free. The incentive may help encourage nodes to stay honest.  If a greedy attacker is able to assemble more CPU power than all the honest nodes, he would have to choose between using it to defraud people by stealing back his payments, or using it to generate new coins.  He ought to find it more profitable to play by the rules, such rules that favour him with more new coins than everyone else combined, than to undermine the system and the validity of his own wealth.\n",
      "\n",
      "7. Reclaiming Disk Space\n",
      "\n",
      "Once the latest transaction in a coin is buried under enough blocks, the spent transactions before it can be discarded to save disk space.  To facilitate this without breaking the block's hash, transactions are hashed in a Merkle Tree [7][2][5], with only the root included in the block's hash. Old blocks can then be compacted by stubbing off branches of the tree.  The interior hashes do not need to be stored. A block header with no transactions would be about 80 bytes.  If we suppose blocks are generated every 10 minutes, 80 bytes * 6 * 24 * 365 = 4.2MB per year.  With computer systems typically selling with 2GB of RAM as of 2008, and Moore's Law predicting current growth of 1.2GB per year, storage should not be a problem even if the block headers must be kept in memory.\n",
      "\n",
      "Block Header (Block Hash) Prev Hash\n",
      "\n",
      "Root Hash\n",
      "\n",
      "Block Header (Block Hash) Root Hash\n",
      "\n",
      "Transactions Hashed in a Merkle Tree\n",
      "\n",
      "After Pruning Tx0-2 from the Block\n",
      "\n",
      "Prev Hash\n",
      "\n",
      "8. Simplified Payment Verification\n",
      "\n",
      "It is possible to verify payments without running a full network node.  A user only needs to keep a copy of the block headers of the longest proof-of-work chain, which he can get by querying network nodes until he's convinced he has the longest chain, and obtain the Merkle branch linking the transaction to the block it's timestamped in.  He can't check the transaction for himself, but by linking it to a place in the chain, he can see that a network node has accepted it, and blocks added after it further confirm the network has accepted it. As such, the verification is reliable as long as honest nodes control the network, but is more vulnerable if the network is overpowered by an attacker.  While network nodes can verify transactions for themselves, the simplified method can be fooled by an attacker's fabricated transactions for as long as the attacker can continue to overpower the network.  One strategy to protect against this would be to accept alerts from network nodes when they detect an invalid block, prompting the user's software to download the full block and alerted transactions to confirm the inconsistency.  Businesses that receive frequent payments will probably still want to run their own nodes for more independent security and quicker verification.\n",
      "\n",
      "9. Combining and Splitting Value\n",
      "\n",
      "Although it would be possible to handle coins individually, it would be unwieldy to make a separate transaction for every cent in a transfer.  To allow value to be split and combined, transactions contain multiple inputs and outputs.  Normally there will be either a single input from a larger previous transaction or multiple inputs combining smaller amounts, and at most two outputs: one for the payment, and one returning the change, if any, back to the sender.  It should be noted that fan-out, where a transaction depends on several transactions, and those transactions depend on many more, is not a problem here.  There is never the need to extract a complete standalone copy of a transaction's history.\n",
      "\n",
      "Transaction\n",
      "\n",
      "Block Header\n",
      "\n",
      "Merkle Root\n",
      "\n",
      "Prev Hash\n",
      "\n",
      "Block Header\n",
      "\n",
      "Merkle Root\n",
      "\n",
      "Prev Hash\n",
      "\n",
      "Block Header\n",
      "\n",
      "Merkle Root\n",
      "\n",
      "Prev Hash\n",
      "\n",
      "Merkle Branch for Tx3\n",
      "\n",
      "Longest Proof-of-Work Chain\n",
      "\n",
      "10. Privacy\n",
      "\n",
      "The traditional banking model achieves a level of privacy by limiting access to information to the parties involved and the trusted third party.  The necessity to announce all transactions publicly precludes this method, but privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous.  The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone.  This is similar to the level of information released by stock exchanges, where the time and size of individual trades, the \"tape\", is made public, but without telling who the parties were. As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner.  Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner.  The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.\n",
      "\n",
      "11. Calculations\n",
      "\n",
      "We consider the scenario of an attacker trying to generate an alternate chain faster than the honest chain.  Even if this is accomplished, it does not throw the system open to arbitrary changes, such as creating value out of thin air or taking money that never belonged to the attacker.  Nodes are not going to accept an invalid transaction as payment, and honest nodes will never accept a block containing them.  An attacker can only try to change one of his own transactions to take back money he recently spent. The race between the honest chain and an attacker chain can be characterized as a Binomial Random Walk.  The success event is the honest chain being extended by one block, increasing its lead by +1, and the failure event is the attacker's chain being extended by one block, reducing the gap by -1. The probability of an attacker catching up from a given deficit is analogous to a Gambler's Ruin problem.  Suppose a gambler with unlimited credit starts at a deficit and plays potentially an infinite number of trials to try to reach breakeven.  We can calculate the probability he ever reaches breakeven, or that an attacker ever catches up with the honest chain, as follows [8]: p = probability an honest node finds the next block q = probability the attacker finds the next block qz = probability the attacker will ever catch up from z blocks behind\n",
      "\n",
      "Identities\n",
      "\n",
      "Transactions\n",
      "\n",
      "Third Party\n",
      "\n",
      "Counterparty\n",
      "\n",
      "Identities\n",
      "\n",
      "Transactions\n",
      "\n",
      "New Privacy Model\n",
      "\n",
      "Traditional Privacy Model\n",
      "\n",
      "Given our assumption that p > q, the probability drops exponentially as the number of blocks the attacker has to catch up with increases.  With the odds against him, if he doesn't make a lucky lunge forward early on, his chances become vanishingly small as he falls further behind. We now consider how long the recipient of a new transaction needs to wait before being sufficiently certain the sender can't change the transaction.  We assume the sender is an attacker who wants to make the recipient believe he paid him for a while, then switch it to pay back to himself after some time has passed.  The receiver will be alerted when that happens, but the sender hopes it will be too late. The receiver generates a new key pair and gives the public key to the sender shortly before signing.  This prevents the sender from preparing a chain of blocks ahead of time by working on it continuously until he is lucky enough to get far enough ahead, then executing the transaction at that moment.  Once the transaction is sent, the dishonest sender starts working in secret on a parallel chain containing an alternate version of his transaction. The recipient waits until the transaction has been added to a block and z blocks have been linked after it.  He doesn't know the exact amount of progress the attacker has made, but assuming the honest blocks took the average expected time per block, the attacker's potential progress will be a Poisson distribution with expected value:\n",
      "\n",
      "To get the probability the attacker could still catch up now, we multiply the Poisson density for each amount of progress he could have made by the probability he could catch up from that point:\n",
      "\n",
      "Rearranging to avoid summing the infinite tail of the distribution\n",
      "\n",
      "Converting to C code double AttackerSuccessProbability(double q, int z)\n",
      "\n",
      "Running some results, we can see the probability drop off exponentially with z.\n",
      "\n",
      "Solving for P less than 0.1% P < 0.001\n",
      "\n",
      "12. Conclusion\n",
      "\n",
      "We have proposed a system for electronic transactions without relying on trust.  We started with the usual framework of coins made from digital signatures, which provides strong control of ownership, but is incomplete without a way to prevent double-spending.  To solve this, we proposed a peer-to-peer network using proof-of-work to record a public history of transactions that quickly becomes computationally impractical for an attacker to change if honest nodes control a majority of CPU power.  The network is robust in its unstructured simplicity.  Nodes work all at once with little coordination.  They do not need to be identified, since messages are not routed to any particular place and only need to be delivered on a best effort basis.  Nodes can leave  and rejoin  the  network  at will, accepting  the  proof-of-work  chain  as proof of what happened while they were gone.  They vote with their CPU power, expressing their acceptance of valid blocks by working on extending them and rejecting invalid blocks by refusing to work on them.  Any needed rules and incentives can be enforced with this consensus mechanism.\n",
      "\n",
      "References\n",
      "\n",
      "[1](W. Dai, \"b-money,\" http://www.weidai.com/bmoney.txt, 1998.)\n",
      "\n",
      "[2](H. Massias, X.S. Avila, and J.-J. Quisquater, \"Design of a secure timestamping service with minimal trust requirements,\" In 20th Symposium on Information Theory in the Benelux, May 1999.)\n",
      "\n",
      "[3](S. Haber, W.S. Stornetta, \"How to time-stamp a digital document,\" In Journal of Cryptology, vol 3, no 2, pages 99-111, 1991.)\n",
      "\n",
      "[4](D. Bayer, S. Haber, W.S. Stornetta, \"Improving the efficiency and reliability of digital time-stamping,\" In Sequences II: Methods in Communication, Security and Computer Science, pages 329-334, 1993.)\n",
      "\n",
      "[5](S. Haber, W.S. Stornetta, \"Secure names for bit-strings,\" In Proceedings of the 4th ACM Conference on Computer and Communications Security, pages 28-35, April 1997.)\n",
      "\n",
      "[6](A. Back, \"Hashcash - a denial of service counter-measure,\" http://www.hashcash.org/papers/hashcash.pdf, 2002.)\n",
      "\n",
      "[7](R.C. Merkle, \"Protocols for public key cryptosystems,\" In Proc. 1980 Symposium on Security and Privacy, IEEE Computer Society, pages 122-133, April 1980.)\n",
      "\n",
      "[8](W. Feller, \"An introduction to probability theory and its applications,\" 1957.)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract('./test/pdf/04-journal.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:1812.09449v3  [cs.CL]  18 Mar 2020\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "A Survey on Deep Learning for\n",
      "\n",
      "Named Entity Recognition\n",
      "\n",
      "Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li\n",
      "\n",
      "Abstract—Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predeﬁned semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in achieving good performance with the cost of human engineering in designing domain-speciﬁc features and rules. In recent years, deep learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing deep learning techniques for NER. We ﬁrst introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then, we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new\n",
      "\n",
      "NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future directions in this area. Index Terms—Natural language processing, named entity recognition, deep learning, survey\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "AMED Entity Recognition (NER) aims to recognize mentions of rigid designators from text belonging to predeﬁned semantic types such as person, location, orga ization etc [1]. NER not only acts as a standalone tool for information extraction (IE), but also plays an essential role in a variety of natural language processing (NLP) applications such as text understanding [2], [3], information retrieval [4], [5], automatic text summarization [6], question answering [7], machine translation [8], and knowledge base construction [9] etc. Evolution of NER. The term “Named Entity” (NE) was as currency, time and percentage expressions. Since MUC  there has been increasing interest in NER, and various scientiﬁc events (e.g., CoNLL03 [11], ACE [12], IREX [13], and TREC Entity Track [14]) devote much effort to this topic. Regarding the problem deﬁnition, Petasis et al. [15](restricted the deﬁnition of named entities: “A NE is a) proper noun, serving as a name for something or someone”. This restriction is justiﬁed by the signiﬁcant percentage of proper nouns present in a corpus. Nadeau and Sekine [1](claimed that the word “Named” restricted the task to only)\n",
      "\n",
      "J. Li is with the Inception Institute of Artiﬁcial Intelligence, United\n",
      "\n",
      "Arab Emirates. This work was done when the author was with Nanyang\n",
      "\n",
      "Technological University, Singapore. E-mail: jli030@e.ntu.edu.sg.\n",
      "\n",
      "A. Sun is with School of Computer Science and Engineering, Nanyang\n",
      "\n",
      "Technological University, Singapore. E-mail: axsun@ntu.edu.sg.\n",
      "\n",
      "J. Han is with SAP, Singapore. E-mail: ray.han@sap.com.\n",
      "\n",
      "C. Li is with School of Cyber Science and Engineering, Wuhan University,\n",
      "\n",
      "China. E-mail:cllee@whu.edu.cn. Accepted in IEEE TKDE. those entities for which one or many rigid designators stands for the referent. Rigid designator, deﬁned in [16], include proper names and natural kind terms like biological species and substances. Despite the various deﬁnitions of NEs, researchers have reached common consensus on the types of NEs to recognize. We generally divide NEs into two categories: generic NEs (e.g., person and location) and domain-speciﬁc NEs (e.g., proteins, enzymes, and genes). In this paper, we mainly focus on generic NEs in English language. We do not claim this article to be exhaustive or representative of all NER works on all languages. As to the techniques applied in NER, there are four main streams: 1) Rule-based approaches, which do not need annotated data as they rely on hand-crafted rules;\n",
      "\n",
      "2) Unsupervised learning approaches, which rely on un upervised algorithms without hand-labeled training ex mples; 3) Feature-based supervised learning approaches, which rely on supervised learning algorithms with careful feature engineering; 4) Deep-learning based approaches, which automatically discover representations needed for the classiﬁcation and/or detection from raw input in an end-to nd manner. We brief 1), 2) and 3), and review 4) in detail. Motivations for conducting this survey. In recent years, deep learning (DL, also named deep neural network) has attracted signiﬁcant attention due to its success in vari us domains. Starting with Collobert et al. [17], DL-based\n",
      "\n",
      "NER systems with minimal feature engineering have been of studies have applied deep learning to NER and succes ively advanced the state-of-the-art performance [17]–[21]. This trend motivates us to conduct a survey to report the current status of deep learning techniques in NER research. By comparing the choices of DL architectures, we aim to identify factors affecting NER performance as well as issues\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020 and challenges. On the other hand, although NER studies have been thriving for a few decades, to the best of our knowledge, there are few reviews in this ﬁeld so far. Arguably the most established one was published by Nadeau and Sekine [1](in\n",
      "\n",
      "2007. This survey presents an overview of the technique) trend from hand-crafted rules towards machine learning. Marrero et al. [22] summarized NER works from the per pectives of fallacies, challenges and opportunities in\n",
      "\n",
      "2013. Then Patawar and Potey [23] provided a short review in\n",
      "\n",
      "2015. The two recent short surveys are on new domains [24](and complex entity mentions [25], respectively. In summary,) existing surveys mainly cover feature-based machine learn ng models, but not the modern DL-based NER systems. More germane to this work are the two recent surveys [26],\n",
      "\n",
      "2018. Goyal et al. [27] surveyed developments and progresses made in NER. However, they did not include recent advances of deep learning techniques. Yadav and\n",
      "\n",
      "Bethard [26] presented a short survey of recent advances in NER based on representations of words in sentence. This survey focuses more on the distributed representations for input (e.g., char- and word-level embeddings) and do not review the context encoders and tag decoders. The recent trend of applied deep learning on NER tasks (e.g., multi ask learning, transfer learning, reinforcement leanring and adversarial learning) are not in their servery as well. Contributions of this survey. We intensely review applica ions of deep learning techniques in NER, to enlighten and guide researchers and practitioners in this area. Speciﬁcally, we consolidate NER corpora, off-the-shelf NER systems provide useful resources for NER research community. We then present a comprehensive survey on deep learning tech iques for NER. To this end, we propose a new taxonomy, which systematically organizes DL-based NER approaches along three axes: distributed representations for input, con ext encoder (for capturing contextual dependencies for tag decoder), and tag decoder (for predicting labels of words in the given sequence). In addition, we also survey the most representative methods for recent applied deep learning techniques in new NER problem settings and applications. Finally, we present readers with the challenges faced by\n",
      "\n",
      "NER systems and outline future directions in this area.\n",
      "\n",
      "BACKGROUND\n",
      "\n",
      "We ﬁrst give a formal formulation of the NER problem. We then introduce the widely-used NER datasets and tools. Next, we detail the evaluation metrics and summarize the traditional approaches to NER.\n",
      "\n",
      "What is NER?\n",
      "\n",
      "A named entity is a word or a phrase that clearly identi es one item from a set of other items that have similar attributes [28]. Examples of named entities are organization, person, and location names in general domain; gene, pro ein, drug and disease names in biomedical domain. NER is the process of locating and classifying named entities in text into predeﬁned entity categories. Formally, sequence\n",
      "\n",
      "1. An illustration of the named entity recognition task. each of which is a named entity mentioned in s. Here,\n",
      "\n",
      "Is ∈ [1, N] and Ie ∈ [1, N] are the start and the end indexes of a named entity mention; t is the entity type from a predeﬁned category set. Figure 1 shows an example where a NER system recognizes three named entities from the given sentence. When NER was ﬁrst deﬁned in MUC-6 [10], the task is to recognize names of people, organizations, locations, and time, currency, percentage expressions in text. Note that the task focuses on a small set of coarse entity types and one type per named entity. We call this kind of\n",
      "\n",
      "NER tasks as coarse-grained NER [10], [11]. Recently, some of entity types where a mention may be assigned multiple variety of downstream applications such as information re rieval, question answering, machine translation, etc. Here, we use semantic search as an example to illustrate the importance of NER in supporting various applications. Se antic search refers to a collection of techniques, which enable search engines to understand the concepts, meaning, and intent behind the queries from users [34]. According to [4], about 71% of search queries contain at least one named entity. Recognizing named entities in search queries would help us to better understand user intents, hence to provide better search results. To incorporate named enti ies in search, entity-based language models [34], which consider individual terms as well as term sequences that have been annotated as entities (both in documents and in queries), have been proposed by Raviv et al. [35]. There are also studies utilizing named entities for an enhanced user experience, such as query recommendation [36], query auto ompletion [37], [38] and entity cards [39], [40].\n",
      "\n",
      "NER Resources: Datasets and Tools\n",
      "\n",
      "High quality annotations are critical for both model learning and evaluation. In the following, we summarize widely sed datasets and off-the-shelf tools for English NER. A tagged corpus is a collection of documents that contain annotations of one or more entity types. Table 1 lists some widely-used datasets with their data sources and number of entity types (also known as tag types). Summarized in Table 1, before 2005, datasets were mainly developed by annotating news articles with a small number of en ity types, suitable for coarse-grained NER tasks. After that, more datasets were developed on various kinds of text sources including Wikipedia articles, conversation, and user-generated text (e.g., tweets and YouTube comments and StackExchange posts in W-NUT). The number of tag\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "List of annotated datasets for English NER. “#Tags” refers to the number of entity types. Corpus\n",
      "\n",
      "Text Source\n",
      "\n",
      "Wall Street Journal https://catalog.ldc.upenn.edu/LDC2003T13\n",
      "\n",
      "MUC-6 Plus\n",
      "\n",
      "Additional news to MUC-6 https://catalog.ldc.upenn.edu/LDC96T10\n",
      "\n",
      "New York Times news https://catalog.ldc.upenn.edu/LDC2001T02\n",
      "\n",
      "Reuters news https://www.clips.uantwerpen.be/conll2003/ner/\n",
      "\n",
      "2000 - 2008\n",
      "\n",
      "Transcripts, news https://www.ldc.upenn.edu/collaborations/past-projects/ace\n",
      "\n",
      "OntoNotes\n",
      "\n",
      "2007 - 2012\n",
      "\n",
      "Magazine, news, web, etc. https://catalog.ldc.upenn.edu/LDC2013T19\n",
      "\n",
      "2015 - 2018\n",
      "\n",
      "User-generated text http://noisy-text.github.io\n",
      "\n",
      "Wall Street Journal https://catalog.ldc.upenn.edu/LDC2005T33\n",
      "\n",
      "WikiGold\n",
      "\n",
      "Wikipedia https://ﬁgshare.com/articles/Learning_multilingual_named_entity\n",
      "\n",
      "Wikipedia http://rali.iro.umontreal.ca/rali/en/winer-wikipedia-for-ner\n",
      "\n",
      "WikiFiger\n",
      "\n",
      "Wikipedia https://github.com/xiaoling/ﬁger\n",
      "\n",
      "Wikipedia https://www.mpi-inf.mpg.de/departments/databases-and nformation-systems/research/yago-naga/hyena http://aksw.org/Projects/N3NERNEDNIF.html\n",
      "\n",
      "Magazine, news, web, etc. https://arxiv.org/e-print/1412.1820v2\n",
      "\n",
      "https://fgner.alt.ai/\n",
      "\n",
      "Newswire https://github.com/nickyringland/nested_named_entities\n",
      "\n",
      "Biology and clinical text http://www.geniaproject.org/home\n",
      "\n",
      "https://sourceforge.net/projects/bioc/ﬁles/\n",
      "\n",
      "FSU-PRGE\n",
      "\n",
      "PubMed and MEDLINE https://julielab.de/Resources/FSU_PRGE.html\n",
      "\n",
      "NCBI-Disease https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/ http://bioc.sourceforge.net/\n",
      "\n",
      "Business news and social media https://dfki-lt-re-group.bitbucket.io/product-corpus/ types becomes signiﬁcantly larger, e.g., 505 in HYENA. We also list a number of domain speciﬁc datasets, particularly developed on PubMed and MEDLINE texts. The number of entity types ranges from 1 in NCBI-Disease to 36 in GENIA. We note that many recent NER works report their perfor ance on CoNLL03 and OntoNotes datasets (see Table 3). CoNLL03 contains annotations for Reuters news in two lan uages: English and German. The English dataset has a large portion of sports news with annotations in four entity types corpus, comprising of various genres (weblogs, news, talk shows, broadcast, usenet newsgroups, and conversational telephone speech) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference). There are\n",
      "\n",
      "5 versions, from Release 1.0 to Release 5.0. The texts are annotated with 18 entity types. We also note two Github repositores1 which host some NER corpora. There are many NER tools available online with pre rained models. Table 2 summarizes popular ones for En lish NER by academia (top) and industry (bottom).\n",
      "\n",
      "NER Evaluation Metrics\n",
      "\n",
      "NER systems are usually evaluated by comparing their outputs against human annotations. The comparison can be quantiﬁed by either exact-match or relaxed match.\n",
      "\n",
      "Exact-match Evaluation\n",
      "\n",
      "NER essentially involves two subtasks: boundary detection and type identiﬁcation. In “exact-match evaluation” [11],\n",
      "\n",
      "[41], [42], a correctly recognized instance requires a system\n",
      "\n",
      "1. https://github.com/juand-r/entity-recognition-datasets https://github.com/cambridgeltl/MTL-Bioinformatics-2016/tree/master/data\n",
      "\n",
      "Off-the-shelf NER tools offered by academia and industry projects. NER System\n",
      "\n",
      "StanfordCoreNLP https://stanfordnlp.github.io/CoreNLP/\n",
      "\n",
      "OSU Twitter NLP https://github.com/aritter/twitter_nlp\n",
      "\n",
      "Illinois NLP http://cogcomp.org/page/software/\n",
      "\n",
      "NeuroNER http://neuroner.com/\n",
      "\n",
      "NERsuite\n",
      "\n",
      "http://nersuite.nlplab.org/\n",
      "\n",
      "Polyglot https://polyglot.readthedocs.io\n",
      "\n",
      "http://bioinformatics.ua.pt/gimli https://spacy.io/api/entityrecognizer\n",
      "\n",
      "https://www.nltk.org https://opennlp.apache.org/\n",
      "\n",
      "LingPipe\n",
      "\n",
      "http://alias-i.com/lingpipe-3.9.3/\n",
      "\n",
      "AllenNLP https://demo.allennlp.org/\n",
      "\n",
      "IBM Watson\n",
      "\n",
      "https://natural-language-understanding emo.ng.bluemix.net https://fgner.alt.ai/extractor/\n",
      "\n",
      "Intellexer http://demo.intellexer.com/\n",
      "\n",
      "Repustate\n",
      "\n",
      "https://repustate.com/named-entity ecognition-api-demo https://developer.aylien.com/text-api-demo\n",
      "\n",
      "Dandelion API https://dandelion.eu/semantic-text/entity xtraction-demo displaCy\n",
      "\n",
      "https://explosion.ai/demos/displacy-ent\n",
      "\n",
      "ParallelDots https://www.paralleldots.com/named ntity-recognition\n",
      "\n",
      "TextRazor https://www.textrazor.com/named\n",
      "\n",
      "to correctly identify its boundary and type, simultaneously. More speciﬁcally, the numbers of False positives (FP), False negatives (FN) and True positives (TP) are used to compute\n",
      "\n",
      "Precision, Recall, and F-score.\n",
      "\n",
      "False Positive (FP): entity that is returned by a NER system but does not appear in the ground truth.\n",
      "\n",
      "False Negative (FN): entity that is not returned by a\n",
      "\n",
      "NER system but appears in the ground truth. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "True Positive (TP): entity that is returned by a NER system and also appears in the ground truth. Precision refers to the percentage of your system results which are correctly recognized. Recall refers to the percent ge of total entities correctly recognized by your system. Precision = harmonic mean of precision and recall, the traditional F easure or balanced F-score:\n",
      "\n",
      "F-score = 2 × Precision × Recall\n",
      "\n",
      "Precision + Recall\n",
      "\n",
      "In addition, the macro-averaged F-score and micro veraged F-score both consider the performance across mul iple entity types. Macro-averaged F-score independently calculates the F-score on different entity types, then takes the average of the F-scores. Micro-averaged F-score sums up the individual false negatives, false positives and true positives across all entity types then applies them to get the statistics. The latter can be heavily affected by the quality of recognizing entities in large classes in the corpus.\n",
      "\n",
      "Relaxed-match Evaluation\n",
      "\n",
      "MUC-6 [10] deﬁnes a relaxed-match evaluation: a correct type is credited if an entity is assigned its correct type regardless its boundaries as long as there is an overlap with ground truth boundaries; a correct boundary is cred ted regardless an entity’s type assignment. Then ACE [12](proposes a more complex evaluation procedure. It resolves a) few issues like partial match and wrong type, and considers subtypes of named entities. However, it is problematic be ause the ﬁnal scores are comparable only when parameters are ﬁxed [1], [22], [23]. Complex evaluation methods are not intuitive and make error analysis difﬁcult. Thus, complex evaluation methods are not widely used in recent studies.\n",
      "\n",
      "Traditional Approaches to NER\n",
      "\n",
      "Traditional approaches to NER are broadly classiﬁed into three main streams: rule-based, unsupervised learning, and feature-based supervised learning approaches [1], [26].\n",
      "\n",
      "Rule-based Approaches\n",
      "\n",
      "Rule-based NER systems rely on hand-crafted rules. Rules can be designed based on domain-speciﬁc gazetteers [9],\n",
      "\n",
      "[43] and syntactic-lexical patterns [44]. Kim [45] proposed to use Brill rule inference approach for speech input. This system generates rules automatically based on Brill’s part f-speech tagger. In biomedical domain, Hanisch et al. [46](proposed ProMiner, which leverages a pre-processed syn nym dictionary to identify protein mentions and potential) gene in biomedical text. Quimbaya et al. [47] proposed a dictionary-based approach for NER in electronic health records. Experimental results show the approach improves recall while having limited impact on precision. Some other well-known rule-based NER systems in lude LaSIE-II [48], NetOwl [49], Facile [50], SAR [51],\n",
      "\n",
      "FASTUS [52], and LTG [53] systems. These systems are mainly based on hand-crafted semantic and syntactic rules to recognize entities. Rule-based systems work very well when lexicon is exhaustive. Due to domain-speciﬁc rules and incomplete dictionaries, high precision and low recall are often observed from such systems, and the systems cannot be transferred to other domains.\n",
      "\n",
      "Unsupervised Learning Approaches\n",
      "\n",
      "A typical approach of unsupervised learning is cluster ng [1]. Clustering-based NER systems extract named en ities from the clustered groups based on context similarity. The key idea is that lexical resources, lexical patterns, and statistics computed on a large corpus can be used to infer mentions of named entities. Collins et al. [54] observed that use of unlabeled data reduces the requirements for supervision to just 7 simple “seed” rules. The authors then presented two unsupervised algorithms for named entity classiﬁcation. Similarly, KNOWITALL [9] leveraged a set of predicate names as input and bootstraps its recognition process from a small set of generic extraction patterns. Nadeau et al. [55] proposed an unsupervised system for gazetteer building and named entity ambiguity resolution. This system combines entity extraction and disambiguation based on simple yet highly effective heuristics. In addi ion, Zhang and Elhadad [44] proposed an unsupervised approach to extracting named entities from biomedical text. Instead of supervision, their model resorts to terminolo ies, corpus statistics (e.g., inverse document frequency and context vectors) and shallow syntactic knowledge (e.g., noun phrase chunking). Experiments on two mainstream biomedical datasets demonstrate the effectiveness and gen ralizability of their unsupervised approach.\n",
      "\n",
      "Feature-based Supervised Learning Approaches\n",
      "\n",
      "Applying supervised learning, NER is cast to a multi-class classiﬁcation or sequence labeling task. Given annotated data samples, features are carefully designed to represent each training example. Machine learning algorithms are then utilized to learn a model to recognize similar patterns from unseen data. Feature engineering is critical in supervised NER sys ems. Feature vector representation is an abstraction over text where a word is represented by one or many Boolean, numeric, or nominal values [1], [56]. Word-level features list lookup features (e.g., Wikipedia gazetteer and DBpedia gazetteer) [60]–[63], and document and corpus features (e.g., local syntax and multiple occurrences) [64]–[67] have been widely used in various supervised NER systems. More feature designs are discussed in [1], [28], [68](Based on these features, many machine learning algo ithms have been applied in supervised NER, including)\n",
      "\n",
      "Hidden Markov Models (HMM) [69], Decision Trees [70],\n",
      "\n",
      "Maximum Entropy Models [71], Support Vector Machines system, named IdentiFinder, to identify and classify names, dates, time expressions, and numerical quantities. In addi ion, Szarvas et al. [76] developed a multilingual NER sys em by using C4.5 decision tree and AdaBoostM1 learning algorithm. A major merit is that it provides an opportunity\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020 to train several independent decision tree classiﬁers through different subsets of features then combine their decisions through a majority voting scheme. Borthwick et al. [77](proposed “maximum entropy named entity” (MENE) by) applying the maximum entropy theory. MENE is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. Other systems using maximum entropy can be found in [78]–[80]. McNamee and Mayﬁeld [81] used 1000 language-related and 258 orthography and punctuation features to train SVM classiﬁers. Each classiﬁer makes binary decision whether the current token belongs to one of the eight classes, i.e.,\n",
      "\n",
      "B- (Beginning), I- (Inside) for PERSON, ORGANIZATION,\n",
      "\n",
      "LOCATION, and MIS tags. SVM does not consider “neigh oring” words when predicting an entity label. CRFs takes context into account. McCallum and Li [82] proposed a fea ure induction method for CRFs in NER. Experiments were performed on CoNLL03, and achieved F-score of 84.04% for English. Krishnan and Manning [67] proposed a two tage approach based on two coupled CRF classiﬁers. The second CRF makes use of the latent representations derived from the output of the ﬁrst CRF. We note that CRF-based\n",
      "\n",
      "NER has been widely applied to texts in various domains, including biomedical text [58], [83], tweets [84], [85] and chemical text [86].\n",
      "\n",
      "DEEP LEARNING TECHNIQUES FOR NER\n",
      "\n",
      "In recent years, DL-based NER models become dominant and achieve state-of-the-art results. Compared to feature ased approaches, deep learning is beneﬁcial in discovering hidden features automatically. Next, we ﬁrst brieﬂy intro uce what deep learning is, and why deep learning for NER. We then survey DL-based NER approaches.\n",
      "\n",
      "Why Deep Learning for NER?\n",
      "\n",
      "Deep learning is a ﬁeld of machine learning that is com osed of multiple processing layers to learn representations of data with multiple levels of abstraction [87]. The typical layers are artiﬁcial neural networks which consists of the forward pass and backward pass. The forward pass com utes a weighted sum of their inputs from the previous layer and pass the result through a non-linear function. The backward pass is to compute the gradient of an objective function with respect to the weights of a multilayer stack of modules via the chain rule of derivatives. The key ad antage of deep learning is the capability of representation learning and the semantic composition empowered by both the vector representation and neural processing. This allows a machine to be fed with raw data and to automatically discover latent representations and processing needed for classiﬁcation or detection [87]. There are three core strengths of applying deep learning techniques to NER. First, NER beneﬁts from the non-linear transformation, which generates non-linear mappings from input to output. Compared with linear models (e.g., log inear HMM and linear chain CRF), DL-based models are able to learn complex and intricate features from data via non-linear activation functions. Second, deep learning saves signiﬁcant effort on designing NER features. The traditional\n",
      "\n",
      "D /)4(2)56(#$%2#02#4#'(!()-'4%7-2%)'06(\n",
      "\n",
      "0AA/)EAA/)F%'.G%.\"),+(\"3/)H!%'><+!,\"!/9\n",
      "\n",
      "8-'(#9(%#',-$#2\n",
      "\n",
      "6+<$,%I/)0EJ/)EAA/) +&'$)'\"$*+!@/9\n",
      "\n",
      "M,-\"((&'.)3%=\"!\n",
      "\n",
      "6\"OG\"'2\"\n",
      "\n",
      "M,-\"((&'.\n",
      "\n",
      "0+'4+3G$&+'\n",
      "\n",
      "01%!%2$\"! 3\"4\"3)\n",
      "\n",
      "2. The taxonomy of DL-based NER. From input sequence to pre icted tags, a DL-based NER model consists of distributed representa ions for input, context encoder, and tag decoder. feature-based approaches require considerable amount of engineering skill and domain expertise. DL-based models, on the other hand, are effective in automatically learning useful representations and underlying factors from raw data. Third, deep neural NER models can be trained in an end-to-end paradigm, by gradient descent. This property enables us to design possibly complex NER systems. Why we use a new taxonomy in this survey? Existing taxonomy [26], [88] is based on character-level encoder, word-level encoder, and tag decoder. We argue that the description of “word-level encoder” is inaccurate because word-level information is used twice in a typical DL-based\n",
      "\n",
      "NER model: 1) word-level representations are used as raw features, and 2) word-level representations (together with character-level representations) are used to capture context dependence for tag decoding. In this survey, we summa ize recent advances in NER with the general architecture presented in Figure\n",
      "\n",
      "2. Distributed representations for input consider word- and character-level embeddings as well as incorporation of additional features like POS tag and gazetteer that have been effective in feature-based based approaches. Context encoder is to capture the context depen encies using CNN, RNN, or other networks. Tag decoder predict tags for tokens in the input sequence. For instance, in Figure 2 each token is predicted with a tag indicated by\n",
      "\n",
      "B-(begin), I-(inside), E-(end), S-(singleton) of a named entity with its type, or O-(outside) of named entities. Note that there are other tag schemes or tag notations, e.g., BIO. Tag decoder may also be trained to detect entity boundaries and then the detected text spans are classiﬁed to the entity types.\n",
      "\n",
      "Distributed Representations for Input\n",
      "\n",
      "A straightforward option of representing a word is one-hot vector representation. In one-hot vector space, two distinct words have completely different representations and are or hogonal. Distributed representation represents words in low dimensional real-valued dense vectors where each dimen ion represents a latent feature. Automatically learned from text, distributed representation captures semantic and syn actic properties of word, which do not explicitly present in\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "0'12')3'\n",
      "\n",
      "7/)8/,29(/)\n",
      "\n",
      "7:!.!39'.;,'8',+\n",
      "\n",
      "0'12')3'\n",
      "\n",
      "7:!.!39'. ,'8',+\n",
      "\n",
      "7/)3!9')!9(/)\n",
      "\n",
      "0'12')3'\n",
      "\n",
      "7/)8/,29(/)\n",
      "\n",
      "7:!.!39'. ,'8',+\n",
      "\n",
      "0'12')3'\n",
      "\n",
      "7:!.!39'.;,'8',+\n",
      "\n",
      "7/)3!9')!9(/)\n",
      "\n",
      "3. CNN-based and RNN-based models for extracting character-level representation for a word. the input to NER. Next, we review three types of distributed representations that have been used in NER models: word evel, character-level, and hybrid representations.\n",
      "\n",
      "Word-level Representation\n",
      "\n",
      "Some studies [89]–[91] employed word-level representation, which is typically pre-trained over large collections of text through unsupervised algorithms such as continuous bag f-words (CBOW) and continuous skip-gram models [92]. Recent studies [88], [93] have shown the importance of such pre-trained word embeddings. Using as the input, the pre-trained word embeddings can be either ﬁxed or further ﬁne-tuned during NER model training. Commonly used word embeddings include Google Word2Vec, Stanford\n",
      "\n",
      "GloVe, Facebook fastText and SENNA. Yao et al. [94] proposed Bio-NER, a biomedical NER model based on deep neural network architecture. The word representation in Bio-NER is trained on PubMed database using skip-gram model. The dictionary contains 205,924 words in 600 dimensional vectors. Nguyen et al. [89] used word2vec toolkit to learn word embeddings for English from the Gigaword corpus augmented with newsgroups data from BOLT (Broad Operational Language Technolo ies). Zhai et al. [95] designed a neural model for sequence chunking, which consists of two sub-tasks: segmentation and labeling. The neural model can be fed with SENNA embeddings or randomly initialized embeddings. Zheng et al. [90] jointly extracted entities and relations using a single model. This end-to-end model uses word embeddings learned on NYT corpus by word2vec tookit. Strubell et al. [91] proposed a tagging scheme based on Iter ted Dilated Convolutional Neural Networks (ID-CNNs). The lookup table in their model are initialized by 100 imensional embeddings trained on SENNA corpus by skip-n-gram. In their proposed neural model for extracting entities and their relations, Zhou et al. [96] used the pre rained 300-dimensional word vectors from Google. In ad ition, GloVe [97], [98] and fastText [99] are also widely used in NER tasks.\n",
      "\n",
      "Character-level Representation\n",
      "\n",
      "Instead of only considering word-level representations as the basic input, several studies [100], [101] incorporated character-based word representations learned from an end o-end neural model. Character-level representation has been found useful for exploiting explicit sub-word-level information such as preﬁx and sufﬁx. Another advantage of character-level representation is that it naturally handles out-of-vocabulary. Thus character-based model is able to in er representations for unseen words and share information of morpheme-level regularities. There are two widely-used architectures for extracting character-level representation:\n",
      "\n",
      "CNN-based and RNN-based models. Figures 3(a) and 3(b) illustrate the two architectures. Ma et al. [97] utilized a CNN for extracting character evel representations of words. Then the character repre entation vector is concatenated with the word embedding before feeding into a RNN context encoder. Likewise, Li et al. [98] applied a series of convolutional and highway layers to generate character-level representations for words. The ﬁnal embeddings of words are fed into a bidirectional recursive network. Yang et al. [102] proposed a neural reranking model for NER, where a convolutional layer with a ﬁxed window-size is used on top of a character embedding layer. Recently, Peters et al. [103] proposed ELMo word representation, which are computed on top of two-layer bidirectional language models with character convolutions. For\n",
      "\n",
      "RNN-based\n",
      "\n",
      "Short-Term character instead of each word. Then word-level tags are ob ained from the character-level tags. Their results show that taking characters as the primary representation is superior to words as the basic input unit. Lample et al. [19] utilized a bidirectional LSTM to extract character-level representa ions of words. Similar to [97], character-level representa ion is concatenated with pre-trained word-level embedding from a word lookup table. Gridach [104] investigated word embeddings and character-level representation in identify ng biomedical named entities. Rei et al. [105] combined character-level representations with word embeddings us ng a gating mechanism. In this way, Rei’s model dynami ally decides how much information to use from a character r word-level component. Tran et al. [101] introduced a neural NER model with stack residual LSTM and trainable bias decoding, where word features are extracted from word embeddings and character-level RNN. Yang et al. [106](developed a model to handle both cross-lingual and multi ask joint training in a uniﬁed manner. They employed a) deep bidirectional GRU to learn informative morphological\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "0'12')3'\n",
      "\n",
      "0'12')3'\n",
      "\n",
      "4. Extraction of a contextual string embedding for word “Washing on” in a sentential context [107]. From the forward language model last character in the word. From the backward language model (shown in blue), the model extracts the output hidden state before the ﬁrst character in the word. Both output hidden states are concatenated to form the ﬁnal embedding of a word.\n",
      "\n",
      "representation from the character sequence of a word. Then character-level representation and word embedding are con atenated to produce the ﬁnal representation for a word. Recent advances in language modeling using recurrent neural networks made it viable to model language as distri utions over characters. The contextual string embeddings by Akbik et al. [107], uses character-level neural language model to generate a contextualized embedding for a string of characters in a sentential context. An important property is that the embeddings are contextualized by their surround ng text, meaning that the same word has different embed ings depending on its contextual use. Figure 4 illustrates the architecture of extracting a contextual string embedding for word “Washington” in a sentential context.\n",
      "\n",
      "Hybrid Representation word-level and character-level representations, some studies also incorporate additional information (e.g., gazetteers [18], [108], lexical similarity [109], linguistic de endency [110] and visual features [111]) into the ﬁnal representations of words, before feeding into context encod ng layers. In other words, the DL-based representation is combined with feature-based approach in a hybrid manner. Adding additional information may lead to improvements in NER performance, with the price of hurting generality of these systems. The use of neural models for NER was pioneered by [17], where an architecture based on temporal convolutional neural networks over word sequence was proposed. When incorporating common priori knowledge (e.g., gazetteers and POS), the resulting system outperforms the baseline using only word-level representations. In the BiLSTM-CRF model by Huang et al. [18], four types of features are used for the NER task: spelling features, context features, word embeddings, and gazetteer features. Their experimen al results show that the extra features (i.e., gazetteers) boost tagging accuracy. The BiLSTM-CNN model by Chiu and Nichols [20] incorporates a bidirectional LSTM and a character-level CNN. Besides word embeddings, the model uses additional word-level features (capitalization, lexicons) and character-level features (4-dimensional vector repre enting the type of a character: upper case, lower case, punctuation, other). Wei et al. [112] presented a CRF-based neural system for recognizing and normalizing disease names. This system\n",
      "\n",
      "4%5,%/2%\n",
      "\n",
      "83+%\"\"'/9\n",
      "\n",
      "6'/%!*?-@!*\"#!/$?-6'/%!*\n",
      "\n",
      "4%5,%/2%-)!99'/9-<!A%*\n",
      "\n",
      "5. Sentence approach network based on CNN [17]. The convo ution layer extracts features from the whole sentence, treating it as a sequence with global structure.\n",
      "\n",
      "employs rich features in addition to word embeddings, including words, POS tags, chunking, and word shape fea ures (e.g., dictionary and morphological features). Strubell et al. [91] concatenated 100-dimensional embeddings with a 5-dimensional word shape vector (e.g., all capitalized, not capitalized, ﬁrst-letter capitalized or contains a capital letter). Lin et al. [113] concatenated character-level repre entation, word-level representation, and syntactical word representation (i.e., POS tags, dependency roles, word posi ions, head positions) to form a comprehensive word repre entation. A multi-task approach for NER was proposed by\n",
      "\n",
      "Aguilar et al. [114]. This approach utilizes a CNN to capture orthographic features and word shapes at character level. For syntactical and contextual information at word level, e.g., POS and word embeddings, the model implements a\n",
      "\n",
      "LSTM architecture. Jansson and Liu [115] proposed to com ine Latent Dirichlet Allocation (LDA) with deep learning on character-level and word-level embeddings. Xu et al. [116] proposed a local detection approach for NER based on ﬁxed-size ordinally forgetting encoding noisy user-generated data like tweets and Snapchat cap ions, word embeddings, character embeddings, and visual features are merged with modality attention. Ghaddar and\n",
      "\n",
      "Langlais [109] found that it was unfair that lexical fea ures had been mostly discarded in neural NER systems. They proposed an alternative lexical representation which is trained ofﬂine and can be added to any neural NER system. The lexical representation is computed for each word with a 120-dimensional vector, where each element encodes the similarity of the word with an entity type. Recently, De lin et al. [119] proposed a new language representation model called BERT, bidirectional encoder representations from transformers. BERT uses masked language models to enable pre-trained deep bidirectional representations. For a given token, its input representation is comprised by summing the corresponding position, segment and token embeddings. Note that pre-trained language model embed ings often require large-scale corpora for training, and in rinsically incorporate auxiliary embeddings (e.g., position and segment embeddings). For this reason, we category these contextualized language-model embeddings as hybrid representations in this survey. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "Context Encoder Architectures\n",
      "\n",
      "Here, we now review widely-used context encoder archi ectures: convolutional neural networks, recurrent neural networks, recursive neural networks, and deep transformer.\n",
      "\n",
      "Convolutional Neural Networks\n",
      "\n",
      "Collobert et al. [17] proposed a sentence approach network where a word is tagged with the consideration of whole sentence, shown in Figure\n",
      "\n",
      "5. Each word in the input se uence is embedded to an N-dimensional vector after the stage of input representation. Then a convolutional layer is used to produce local features around each word, and the size of the output of the convolutional layers depends on the number of words in the sentence. The global feature vector is constructed by combining local feature vectors extracted by the convolutional layers. The dimension of the global feature vector is ﬁxed, independent of the sentence length, in order to apply subsequent standard afﬁne layers. Two approaches are widely used to extract global features: a max or an averaging operation over the position (i.e., features are fed into tag decoder to compute distribution scores for all possible tags for the words in the network input. Following Collobert’s work, Yao et al. [94] proposed\n",
      "\n",
      "Bio-NER for biomedical NER. Wu et al. [120] utilized a convolutional layer to generate global features represented by a number of global hidden nodes. Both local features and global features are then fed into a standard afﬁne network to recognize named entities in clinical text. Zhou et al. [96] observed that with RNN latter words inﬂuence the ﬁnal sentence representation more than former words. However, important words may appear anywhere in a sentence. In their proposed model, named BLSTM E, BLSTM is used to capture long-term dependencies and obtain the whole representation of an input sequence. CNN is then utilized to learn a high-level representation, which is then fed into a sigmoid classiﬁer. Finally, the whole sentence representation (generated by BLSTM) and the relation pre entation (generated by the sigmoid classiﬁer) are fed into another LSTM to predict entities. Traditionally, the time complexity of LSTMs for a se uence of length N is O(N) in a parallelism manner. Strubell et al. [91] proposed ID-CNNs, referred to Iterated\n",
      "\n",
      "Dilated Convolutional Neural Networks, which is more computationally efﬁcient due to the capacity of handling larger context and structured prediction. Figure 6 shows the architecture of a dilated CNN block, where four stacked dilated convolutions of width 3 produce token represen ations. Experimental results show that ID-CNNs achieves\n",
      "\n",
      "14-20x test-time speedups compared to Bi-LSTM-CRF while retaining comparable accuracy.\n",
      "\n",
      "Recurrent Neural Networks\n",
      "\n",
      "Recurrent neural networks, together with its variants such as gated recurrent unit (GRU) and long-short term memory modeling sequential data. In particular, bidirectional RNNs efﬁciently make use of past information (via forward states) and future information (via backward states) for a speciﬁc time frame [18]. Thus, a token encoded by a bidirectional the effective input width of the network: the size of the input context which is observed, directly or indirectly, by the representation of a token at a given layer in the network. Speciﬁcally, in a network composed of a series of stacked convo utional layers of convolution width of context tokens incorporated into a to en’s representation at a given layer , is given by\n",
      "\n",
      "1. The number of layers required to incorporate the entire input context grows lin arly with the length of the sequence. To avoid this scaling, one could pool representations across the sequence, but this is not appropriate for sequence labeling, since it reduces the output resolution of the representation. In response, this paper presents an application of dilated convolutions Yu and Koltun 2016) for sequence labeling (Figure\n",
      "\n",
      "parameters to estimate. Like typical CNN layers, dilated convolutions operate on a sliding window of context over the sequence, but unlike conven ional convolutions, the context need not be con ecutive; the dilated window skips over every dila ion width inputs. By stacking layers of dilated\n",
      "\n",
      "convolutions of exponentially increasing dilation width, we can expand the size of the effective input width to cover the entire length of most sequences using only a few layers: The size of the effective input width for a token at layer\n",
      "\n",
      "is now given by convolutions of width 3 produces token represen ations with a n effective input width of 31 tokens the Penn TreeBank. Our overall iterated dilated CNN architecture also provides opportunities to inject supervision on intermediate activations of the network. Simi ar to models that use logits produced by an RNN, the ID-CNN provides two methods for perform ng prediction: we can predict each token’s label independently, or by running Viterbi inference in a chain structured graphical model. In experiments on CoNLL 2003 and OntoNotes\n",
      "\n",
      "What we call effective input width here is known as the receptive ﬁeld in the vision literature, drawing an analogy to the visual receptive ﬁeld of a neuron in the retina. Figure 1: A dilated CNN block with maximum dilation width 4 and ﬁlter width\n",
      "\n",
      "3. Neurons con ributing to a single highlighted neuron in the last layer are also highlighted.\n",
      "\n",
      "5.0 English NER, we demonstrate signiﬁcant speed gains of our ID-CNNs over various recur ent models, while maintaining similar F1 perfor ance. When performing prediction using inde endent classiﬁcation, the ID-CNN consistently outperforms a bidirectional LSTM (Bi-LSTM), and performs on par with inference in a CRF with logits from a Bi-LSTM (Bi-LSTM-CRF). As an extractor of per-token logits for a CRF, our model out-performs the Bi-LSTM-CRF. We also apply ID-CNNs to entire documents, where inde endent token classiﬁcation is as accurate as the\n",
      "\n",
      "Bi-LSTM-CRF while decoding almost 8 faster. The clear accuracy gains resulting from incorpo ating broader context suggest that these mod ls could similarly beneﬁt many other context ensitive NLP tasks which have until now been limited by the computational complexity of exist ng context-rich models.\n",
      "\n",
      "Background\n",
      "\n",
      "Conditional Probability Models for be our input text and\n",
      "\n",
      "be per-token output tags. Let the domain size of each\n",
      "\n",
      "likely , given a conditional model\n",
      "\n",
      "This paper considers two factorizations of the conditional distribution. First, we have\n",
      "\n",
      "where the tags are conditionally independent given some features for\n",
      "\n",
      "x. Given these features, prediction is simple and parallelizable across the\n",
      "\n",
      "Our implementation in TensorFlow (Abadi et al.\n",
      "\n",
      "2015) is available at: https://github.com/iesl/\n",
      "\n",
      "dilated-cnn-ner\n",
      "\n",
      "6. The architecture of ID-CNNs with ﬁlter width 3 and maximum dilation width\n",
      "\n",
      "4. [91].\n",
      "\n",
      "4'2$!%5 6%11*%7\n",
      "\n",
      "60*\"!/ 8!(\n",
      "\n",
      "9*00:57/\n",
      "\n",
      "7. The architecture of RNN-based context encoder. RNN will contain evidence from the whole input sentence. Bidirectional RNNs therefore become de facto standard for composing deep context-dependent representations of text [91], [97]. A typical architecture of RNN-based context encoder is shown in Figure\n",
      "\n",
      "7. The work by Huang et al. [18] is among the ﬁrst to utilize a bidirectional LSTM CRF architecture to sequence tagging tasks (POS, chunking and NER). Following [18], a body of works [19], [20], [89], [90], [95]–[97], [101], [105], [112], [113](applied BiLSTM as the basic architecture to encode sequence) context information. Yang et al. [106] employed deep GRUs on both character and word levels to encode morphology and context information. They further extended their model to cross-lingual and multi-task joint trained by sharing the architecture and parameters. Gregoric et al. [121] employed multiple independent bidirectional LSTM units across the same input. Their model promotes diversity among the LSTM units by employing an inter-model regularization term. By distributing computa ion across multiple smaller LSTMs, they found a reduc ion in total number of parameters. Recently, some stud es [122], [123] designed LSTM-based neural networks for nested named entity recognition. Katiyar and Cardie [122](presented a modiﬁcation to standard LSTM-based sequence) labeling model to handle nested named entity recognition. Ju et al. [123] proposed a neural model to identify nested entities by dynamically stacking ﬂat NER layers until no outer entities are extracted. Each ﬂat NER layer employs bidirectional LSTM to capture sequential context. The model merges the outputs of the LSTM layer in the current ﬂat\n",
      "\n",
      "NER layer to construct new representations for the detected entities and then feeds them into the next ﬂat NER layer.\n",
      "\n",
      "Recursive Neural Networks\n",
      "\n",
      "Recursive neural networks are non-linear adaptive mod ls that are able to learn deep structured information, by traversing a given structure in topological order. Named\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "8. Bidirectional recursive neural networks for NER [98]. The com utations are done recursively in two directions. The bottom-up direction computes the semantic composition of the subtree of each node, and the top-down counterpart propagates to that node the linguistic structures which contain the subtree.\n",
      "\n",
      "0(101#'#\n",
      "\n",
      "0(101#'#\n",
      "\n",
      "3'4#5('#\n",
      "\n",
      "3'4#5('#\n",
      "\n",
      "0(101#'# 2\n",
      "\n",
      "9. A sequence labeling model with an additional language modeling objective [124], performing NER on the sentence “Fischler proposes measures”. At each token position (e.g., “proposes”), the network is optimised to predict the previous word (“Fischler”), the current label entities are highly related to linguistic constituents, e.g., noun phrases [98]. However, typical sequential labeling approaches take little into consideration about phrase struc ures of sentences. To this end, Li et al. [98] proposed to classify every node in a constituency structure for NER. This model recursively calculates hidden state vectors of every node and classiﬁes each node by these hidden vectors. Fig re 8 shows how to recursively compute two hidden state features for every node. The bottom-up direction calculates the semantic composition of the subtree of each node, and the top-down counterpart propagates to that node the lin uistic structures which contain the subtree. Given hidden vectors for every node, the network calculates a probability distribution of entity types plus a special non-entity type.\n",
      "\n",
      "Neural Language Models\n",
      "\n",
      "Language model is a family of models describing the gener tion of sequences. Given a token sequence, (t1, t2, a forward language model computes the probability of the sequence by modeling the probability of token tk given its history (t1,\n",
      "\n",
      "p(t1, t2,\n",
      "\n",
      "p(tk|t1, t2,\n",
      "\n",
      "order, predicting the previous token given its future context: p(t1, t2,\n",
      "\n",
      "p(tk|tk+1, tk+2, , tN)\n",
      "\n",
      "8):/3(');&#6!)/\n",
      "\n",
      "10. Sequence labeling architecture with contextualized representa ions [125]. Character-level representation, pre-trained word embedding and contextualized representation from bidirectional language models are concatenated and further fed into context encoder. For neural language models, probability of token tk can be computed by the output of recurrent neural networks. At each position k, we can obtain two context-dependent representations (forward and backward) and then combine them as the ﬁnal language model embedding for token tk. Such language-model-augmented knowledge has been empirically veriﬁed to be helpful in numerous sequence labeling tasks [21], [103], [124]–[127]. Rei [124] proposed a framework with a secondary objec ive – learning to predict surrounding words for each word in the dataset. Figure 9 illustrates the architecture with a short sentence on the NER task. At each time step (i.e., token position), the network is optimised to predict the previous token, the current tag, and the next token in the sequence. The added language modeling objective encourages the system to learn richer feature representations which are then reused for sequence labeling. Peters et al. [21] proposed TagLM, a language model augmented sequence tagger. This tagger considers both pre-trained word embeddings and bidirectional language model embeddings for every token in the input sequence for sequence labeling task. Figure 10 shows the architecture of\n",
      "\n",
      "LM-LSTM-CRF model [125], [126]. The language model and sequence tagging model share the same character-level layer in a multi-task learning manner. The vectors from character evel embeddings, pre-trained word embeddings, and lan uage model representations, are concatenated and fed into the word-level LSTMs. Experimental results demonstrate that multi-task learning is an effective approach to guide the language model to learn task-speciﬁc knowledge. Figure 4 shows the contextual string embedding us ng neural character-level language modeling by Akbik et al. [107]. They utilized the hidden states of a forward ackward recurrent neural network to create contextual zed word embeddings. A major merit of this model is that character-level language model is independent of to enization and a ﬁxed vocabulary. Peters et al. [103] pro osed ELMo representations, which are computed on top of two-layer bidirectional language models with character convolutions. This new type of deep contextualized word representation is capable of modeling both complex char cteristics of word usage (e.g., semantics and syntax), and usage variations across linguistic contexts (e.g., polysemy). IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "11. Differences in pre-training model architectures [119]. Google BERT uses a bidirectional Transformer (abbreviated as “Trm”). OpenAI GPT uses a left-to-right Transformer. AllenNLP ELMo uses the concatenation of independently trained left-to-right and right-to-left LSTM to generate features for downstream tasks.\n",
      "\n",
      "Deep Transformer\n",
      "\n",
      "Neural sequence labeling models are typically based on complex convolutional or recurrent networks which consists of encoders and decoders. Transformer, proposed by Vaswani et al. [128], dispenses with recurrence and convolutions en irely. Transformer utilizes stacked self-attention and point ise, fully connected layers to build basic blocks for encoder and decoder. Experiments on various tasks [128]–[130] show\n",
      "\n",
      "Transformers to be superior in quality while requiring sig iﬁcantly less time to train. Based on transformer, Radford et al. [131] proposed\n",
      "\n",
      "Generative Pre-trained Transformer (GPT) for language un erstanding tasks. GPT has a two-stage training procedure. First, they use a language modeling objective with Trans ormers on unlabeled data to learn the initial parameters. Then they adapt these parameters to a target task using the supervised objective, resulting in minimal changes to the pre-trained model. Unlike GPT (a left-to-right architecture),\n",
      "\n",
      "Bidirectional Encoder Representations from Transformers\n",
      "\n",
      "GPT [131] and ELMo [103]. In addition, Baevski et al. [132](proposed a novel cloze-driven pre-training regime based on) a bi-directional Transformer, which is trained with a cloze tyle objective and predicts the center word given all left and right context. These language model embeddings pre-trained using\n",
      "\n",
      "Transformer are becoming a new paradigm of NER. First, these embeddings are contextualized and can be used to re lace the traditional embeddings, such as Google Word2vec and Stanford GloVe. Some studies [108], [110], [133]–[136](have achieved promising performance via leveraging the) combination of traditional embeddings and language model embeddings. Second, these language model embeddings can be further ﬁne-tuned with one additional output layer for a wide range of tasks including NER and chunking. Especially, Li et al. [137], [138] framed the NER task as a machine reading comprehension (MRC) problem, which can be solved by ﬁne-tuning the BERT model.\n",
      "\n",
      "Tag Decoder Architectures\n",
      "\n",
      "Tag decoder is the ﬁnal stage in a NER model. It takes context-dependent representations as input and produce a sequence of tags corresponding to the input sequence. Figure 12 summarizes four architectures of tag decoders:\n",
      "\n",
      "MLP + softmax layer, conditional random ﬁelds (CRFs), recurrent neural networks, and pointer networks.\n",
      "\n",
      "Multi-Layer Perceptron + Softmax\n",
      "\n",
      "NER is in general formulated as a sequence labeling prob em. With a multi-layer Perceptron + Softmax layer as the tag decoder layer, the sequence labeling task is cast as a multi-class classiﬁcation problem. Tag for each word is independently predicted based on the context-dependent representations without taking into account its neighbors. A number of NER models [91], [98], [116], [119], [139](that have been introduced earlier use MLP + Softmax as) the tag decoder. As a domain-speciﬁc NER task, Tomori et al. [140] used softmax as tag decoder to predict game states in Japanese chess game. Their model takes both input from text and input from chess board (9 × 9 squares with 40 pieces of 14 different types) and predict 21 named entities speciﬁc to this game. Text representations and game state embeddings are both fed to a softmax layer for prediction of named entities using BIO tag scheme.\n",
      "\n",
      "Conditional Random Fields\n",
      "\n",
      "A conditional random ﬁeld (CRF) is a random ﬁeld globally conditioned on the observation sequence [73]. CRFs have been widely used in feature-based supervised learning ap roaches (see Section 2.4.3). Many deep learning based NER models use a CRF layer as the tag decoder, e.g., on top of an bidirectional LSTM layer [18], [90], [103], [141], and on top of a CNN layer [17], [91], [94]. Listed in Table 3, CRF is the most common choice for tag decoder, and the state-of-the rt performance on CoNLL03 and OntoNotes5.0 is achieved by [107] with a CRF tag decoder. CRFs, however, cannot make full use of segment-level information because the inner properties of segments cannot be fully encoded with word-level representations. Zhuo et al. [142] then proposed gated recursive semi-markov\n",
      "\n",
      "CRFs, which directly model segments instead of words, and automatically extract segment-level features through a gated recursive convolutional neural network. Recently,\n",
      "\n",
      "Ye and Ling [143] proposed hybrid semi-Markov CRFs for neural sequence labeling. This approach adopts segments instead of words as the basic units for feature extraction and transition modeling. Word-level labels are utilized in\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "6+) !\"#$%&7'%(()%*7'+),$ $5\n",
      "\n",
      "6+) !\"#$%&7'%(()%*7'+),$ $5\n",
      "\n",
      "6+) !\"#$%&7'%(()%*7'+),$ $5\n",
      "\n",
      "6+) !\"#$%&7'%(()%*7'+),$ $5\n",
      "\n",
      "12. Differences in four tag decoders: MLP+Softmax, CRF, RNN, and Pointer network. deriving segment scores. Thus, this approach is able to lever ge both word- and segment-level information for segment score calculation.\n",
      "\n",
      "Recurrent Neural Networks\n",
      "\n",
      "A few studies [88]–[90], [96], [144] have explored RNN to decode tags. Shen et al. [88] reported that RNN tag decoders outperform CRF and are faster to train when the number of entity types is large. Figure 12(c) illustrates the workﬂow of\n",
      "\n",
      "RNN-based tag decoders, which serve as a language model to greedily produce a tag sequence. The [GO]-symbol at the ﬁrst step is provided as y1 to the RNN decoder. Sub equently, at each time step i, the RNN decoder computes current decoder hidden state hDec\n",
      "\n",
      "i+1 in terms of previous step tag yi, previous step decoder hidden state hDec and current\n",
      "\n",
      "step encoder hidden state hEnc i+1 ; the current output tag yi+1\n",
      "\n",
      "is decoded by using a softmax loss function and is further fed as an input to the next time step. Finally, we obtain a tag sequence over all time steps.\n",
      "\n",
      "Pointer Networks\n",
      "\n",
      "Pointer networks apply RNNs to learn the conditional prob bility of an output sequence with elements that are dis rete tokens corresponding to the positions in an input se uence [145], [146]. It represents variable length dictionaries by using a softmax probability distribution as a “pointer”. Zhai et al. [95] ﬁrst applied pointer networks to produce sequence tags. Illustrated in Figure 12(d), pointer networks operation is repeated until all the words in input sequence are processed. In Figure 12(d), given the start token “<s>”, the segment “Michael Jeffery Jordan” is ﬁrst identiﬁed and then labeled as “PERSON”. The segmentation and labeling can be done by two separate neural networks in pointer networks. Next, “Michael Jeffery Jordan” is taken as input and fed into pointer networks. As a result, the segment\n",
      "\n",
      "Summary of DL-based NER\n",
      "\n",
      "Architecture Summary. Table 3 summarizes recent works on neural NER by their architecture choices. BiLSTM-CRF is the most common architecture for NER using deep learning. The method [132] which pre-trains a bi-directional Trans ormer model in a cloze-style manner, achieves the state-of he-art performance (93.5%) on CoNLL03. The work with\n",
      "\n",
      "BERT and dice loss [138] achieves the state-of-the-art per ormance (92.07%) on OntoNotes5.0. The success of a NER system heavily relies on its input representation. Integrating or ﬁne-tuning pre-trained lan uage model embeddings is becoming a new paradigm for neural NER. When leveraging these language model em eddings, there are signiﬁcant performance improvements\n",
      "\n",
      "[103], [107], [108], [132]–[138]. The last column in Table 3 lists the reported performance in F-score on a few bench ark datasets. While high F-scores have been reported on formal documents (e.g., CoNLL03 and OntoNotes5.0), NER on noisy data (e.g., W-NUT17) remains challenging. Architecture Comparison. We discuss pros and cons from three perspectives: input, encoder, and decoder. First, no consensus has been reached about whether external knowl dge should be or how to integrate into DL-based NER models. Some studies [108], [110], [133], [142] shows that\n",
      "\n",
      "NER performance can be boosted with external knowledge. However, the disadvantages are also apparent: 1) acquiring external knowledge is labor-intensive (e.g., gazetteers) or computationally expensive (e.g., dependency); 2) integrat ng external knowledge adversely affects end-to-end learn ng and hurts the generality of DL-based systems. Second, Transformer encoder is more effective than\n",
      "\n",
      "LSTM when Transformer is pre-trained on huge corpora. Transformers fail on NER task if they are not pre-trained and when the training data is limited [147], [148]. On the other hand, Transformer encoder is faster than recursive layers when the length of the sequence n is smaller than the dimensionality of the representation d (complexities: self ttention O(n2 · d) and recurrent O(n · d2)) [128]. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "Summary of recent works on neural NER. LSTM: long short-term memory, CNN: convolutional neural network, GRU: gated recurrent unit, LM: language model, ID-CNN: iterated dilated convolutional neural network, BRNN: bidirectional recursive neural network, MLP: multi-layer perceptron,\n",
      "\n",
      "CRF: conditional random ﬁeld, Semi-CRF: Semi-markov conditional random ﬁeld, FOFE: ﬁxed-size ordinally forgetting encoding. Work\n",
      "\n",
      "Input representation\n",
      "\n",
      "Tag decoder\n",
      "\n",
      "Performance (F-score) Character\n",
      "\n",
      "[94]( rained on PubMed)\n",
      "\n",
      "GENIA: 71.01%\n",
      "\n",
      "[89]( rained on Gigaword)\n",
      "\n",
      "ACE 2005: 80.00%\n",
      "\n",
      "[95]( andom)\n",
      "\n",
      "Pointer Network\n",
      "\n",
      "ATIS: 96.86%\n",
      "\n",
      "[90]( rained on NYT)\n",
      "\n",
      "NYT: 49.50%\n",
      "\n",
      "[91]( ENNA)\n",
      "\n",
      "Word shape\n",
      "\n",
      "CoNLL03: 90.65%;\n",
      "\n",
      "OntoNotes5.0: 86.84%\n",
      "\n",
      "[96]( oogle word2vec)\n",
      "\n",
      "CoNLL04: 75.0%\n",
      "\n",
      "[100](LSTM)\n",
      "\n",
      "CoNLL03: 84.52%\n",
      "\n",
      "[97](CNN)\n",
      "\n",
      "CoNLL03: 91.21%\n",
      "\n",
      "[105](LSTM)\n",
      "\n",
      "Google word2vec\n",
      "\n",
      "CoNLL03: 84.09%\n",
      "\n",
      "[19](LSTM)\n",
      "\n",
      "CoNLL03: 90.94%\n",
      "\n",
      "[106](GRU)\n",
      "\n",
      "CoNLL03: 90.94%\n",
      "\n",
      "[98](CNN)\n",
      "\n",
      "OntoNotes5.0: 87.21%\n",
      "\n",
      "[107](LSTM-LM)\n",
      "\n",
      "CoNLL03: 93.09%;\n",
      "\n",
      "OntoNotes5.0: 89.71%\n",
      "\n",
      "[103](CNN-LSTM-LM)\n",
      "\n",
      "CoNLL03: 92.22%\n",
      "\n",
      "[17]( andom)\n",
      "\n",
      "CoNLL03: 89.86%\n",
      "\n",
      "[18]( ENNA)\n",
      "\n",
      "Spelling, n-gram, gazetteer\n",
      "\n",
      "CoNLL03: 90.10%\n",
      "\n",
      "[20](CNN) capitalization, lexicons\n",
      "\n",
      "CoNLL03: 91.62%;\n",
      "\n",
      "OntoNotes5.0: 86.34%\n",
      "\n",
      "[116]( )\n",
      "\n",
      "CoNLL03: 91.17%\n",
      "\n",
      "[101](LSTM)\n",
      "\n",
      "CoNLL03: 91.07%\n",
      "\n",
      "[113](LSTM)\n",
      "\n",
      "Syntactic\n",
      "\n",
      "W-NUT17: 40.42%\n",
      "\n",
      "[102](CNN)\n",
      "\n",
      "Reranker\n",
      "\n",
      "CoNLL03: 91.62%\n",
      "\n",
      "[114](CNN)\n",
      "\n",
      "Twitter Word2vec\n",
      "\n",
      "W-NUT17: 41.86%\n",
      "\n",
      "[115](LSTM)\n",
      "\n",
      "POS, topics\n",
      "\n",
      "W-NUT17: 41.81%\n",
      "\n",
      "[118](LSTM)\n",
      "\n",
      "SnapCaptions: 52.4%\n",
      "\n",
      "[109](LSTM)\n",
      "\n",
      "CoNLL03: 91.73%;\n",
      "\n",
      "OntoNotes5.0: 87.95%\n",
      "\n",
      "[119]( ordPiece)\n",
      "\n",
      "Segment, position\n",
      "\n",
      "Transformer\n",
      "\n",
      "CoNLL03: 92.8%\n",
      "\n",
      "[121](LSTM)\n",
      "\n",
      "CoNLL03: 91.48%\n",
      "\n",
      "[124](LSTM)\n",
      "\n",
      "Google Word2vec\n",
      "\n",
      "CoNLL03: 86.26%\n",
      "\n",
      "[21](GRU)\n",
      "\n",
      "CoNLL03: 91.93%\n",
      "\n",
      "[126](LSTM)\n",
      "\n",
      "CoNLL03: 91.71%\n",
      "\n",
      "[142]( ENNA)\n",
      "\n",
      "POS, gazetteers\n",
      "\n",
      "Semi-CRF\n",
      "\n",
      "CoNLL03: 90.87%\n",
      "\n",
      "[143](LSTM)\n",
      "\n",
      "Semi-CRF\n",
      "\n",
      "CoNLL03: 91.38%\n",
      "\n",
      "[88](CNN)\n",
      "\n",
      "Trained on Gigaword\n",
      "\n",
      "CoNLL03: 90.69%;\n",
      "\n",
      "OntoNotes5.0: 86.15%\n",
      "\n",
      "[110]( loVe)\n",
      "\n",
      "ELMo, dependency\n",
      "\n",
      "CoNLL03: 92.4%;\n",
      "\n",
      "OntoNotes5.0: 89.88%\n",
      "\n",
      "[108](CNN)\n",
      "\n",
      "ELMo, gazetteers\n",
      "\n",
      "Semi-CRF\n",
      "\n",
      "CoNLL03: 92.75%;\n",
      "\n",
      "OntoNotes5.0: 89.94%\n",
      "\n",
      "[133](LSTM)\n",
      "\n",
      "ELMo, POS\n",
      "\n",
      "CoNLL03: 92.28%\n",
      "\n",
      "[137]( )\n",
      "\n",
      "CoNLL03: 93.04%;\n",
      "\n",
      "OntoNotes5.0: 91.11%\n",
      "\n",
      "[138]( )\n",
      "\n",
      "CoNLL03: 93.33%;\n",
      "\n",
      "OntoNotes5.0: 92.07%\n",
      "\n",
      "[134](LSTM)\n",
      "\n",
      "BERT, document-level em eddings\n",
      "\n",
      "CoNLL03: 93.37%;\n",
      "\n",
      "OntoNotes5.0: 90.3%\n",
      "\n",
      "[135](CNN)\n",
      "\n",
      "BERT, global embeddings\n",
      "\n",
      "CoNLL03: 93.47%\n",
      "\n",
      "[132](CNN)\n",
      "\n",
      "CoNLL03: 93.5%\n",
      "\n",
      "[136]( loVe)\n",
      "\n",
      "Plooled contextual embed ings\n",
      "\n",
      "CoNLL03: 93.47%\n",
      "\n",
      "Third, a major disadvantage of RNN and Pointer Net ork decoders lies in greedily decoding, which means that the input of current step needs the output of previous step. This mechanism may have a signiﬁcant impact on the speed and is an obstacle to parallelization. CRF is the most common choice for tag decoder. CRF is powerful to capture label transition dependencies when adopting non anguage-model (i.e., non-contextualized) embeddings such as Word2vec and GloVe. However, CRF could be compu ationally expensive when the number of entity types is large. More importantly, CRF does not always lead to better performance compared with softmax classiﬁcation when adopting contextualized language model embeddings such as BERT and ELMo [137], [139]. For end users, what architecture to choose is data and domain task dependent. If data is abundant, training models with RNNs from scratch and ﬁne-tuning contextualized language models could be considered. If data is scarce, adopting transfer strategies might be a better choice. For newswires domain, there are many pre-trained off-the-shelf models available. For speciﬁc domains (e.g., medical and social media), ﬁne-tuning general-purpose contextualized language models with domain-speciﬁc data is often an effective way. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "NER for Different Languages. In this survey, we mainly focus on NER in English and in general domain. Apart from English language, there are many studies on other languages or cross-lingual settings. Wu et al. [120] and\n",
      "\n",
      "Wang et al. [149] investigated NER in Chinese clinical text. Zhang and Yang [150] proposed a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon. Other than Chinese, many studies are conducted on other languages. Examples include Mongolian [151], Czech [152],\n",
      "\n",
      "Arabic [153], Urdu [154], Vietnamese [155], Indonesian [156], and Japanese [157]. Each language has its own characteris ics for understanding the fundamentals of NER task on that language. There are also a number of studies [106], [158]–\n",
      "\n",
      "[160] aiming to solve the NER problem in a cross-lingual setting by transferring knowledge from a source language to a target language with few or no labels.\n",
      "\n",
      "APPLIED DEEP LEARNING FOR NER\n",
      "\n",
      "Sections 3.2, 3.3, and 3.4 outline typical network architec ures for NER. In this section, we survey recent applied deep learning techniques that are being explored for NER.\n",
      "\n",
      "Deep Multi-task Learning for NER\n",
      "\n",
      "Multi-task learning [161] is an approach that learns a group of related tasks together. By considering the relation be ween different tasks, multi-task learning algorithms are expected to achieve better results than the ones that learn each task individually. Collobert et al. [17] trained a window/sentence ap roach network to jointly perform POS, Chunk, NER, and\n",
      "\n",
      "SRL tasks. This multi-task mechanism lets the training algo ithm to discover internal representations that are useful for all the tasks of interest. Yang et al. [106] proposed a multi ask joint model, to learn language-speciﬁc regularities, jointly trained for POS, Chunk, and NER tasks. Rei [124](found that by including an unsupervised language model ng objective in the training process, the sequence labeling) model achieves consistent performance improvement. Lin et al. [160] proposed a multi-lingual multi-task architecture for low-resource settings, which can effectively transfer dif erent types of knowledge to improve the main model. Other than considering NER together with other se uence labeling tasks, multi-task learning framework can be applied for joint extraction of entities and relations [90],\n",
      "\n",
      "[96], or to model NER as two related subtasks: entity seg entation and entity category prediction [114], [162]. In biomedical domain, because of the differences in different datasets, NER on each dataset is considered as a task in a multi-task setting [163], [164]. A main assumption here is that the different datasets share the same character- and word-level information. Then multi-task learning is applied to make more efﬁcient use of the data and to encourage the models to learn more generalized representations.\n",
      "\n",
      "Deep Transfer Learning for NER\n",
      "\n",
      "Transfer learning aims to perform a machine learning task on a target domain by taking advantage of knowledge learned from a source domain [165]. In NLP, transfer learning is also known as domain adaptation. On NER tasks, the traditional approach is through bootstrapping algorithms [166]–[168]. Recently, a few approaches [127],\n",
      "\n",
      "[169]–[173] have been proposed for low-resource and across omain NER using deep neural networks. Pan et al. [169] proposed a transfer joint embedding embedding techniques to transform multi-class classiﬁca ion to regression in a low-dimensional latent space. Qu et al. [174] observed that related named entity types often share lexical and context features. Their approach learns the correlation between source and target named entity types using a two-layer neural network. Their approach is appli able to the setting that the source domain has similar (but not identical) named entity types with the target domain. Peng and Dredze [162] explored transfer learning in a multi ask learning setting, where they considered two domains: news and social media, for two tasks: word segmentation and NER. In the setting of transfer learning, different neural mod ls commonly share different parts of model parameters between source task and target task. Yang et al. [175] ﬁrst investigated the transferability of different layers of repre entations. Then they presented three different parameter haring architectures for cross-domain, cross-lingual, and cross-application scenarios. If two tasks have mappable label sets, there is a shared CRF layer, otherwise, each task learns a separate CRF layer. Experimental results show signiﬁcant improvements on various datasets under low esource conditions (i.e., fewer available annotations). Pius and Mark [176] extended Yang’s approach to allow joint training on informal corpus (e.g., WNUT 2017), and to incor orate sentence level feature representation. Their approach achieved the 2nd place at the WNUT 2017 shared task for\n",
      "\n",
      "NER, obtaining an F1-score of 40.78%. Zhao et al. [177] pro osed a multi-task model with domain adaption, where the fully connection ayer are adapted to different datasets, and the CRF features are computed separately. A major merit of\n",
      "\n",
      "Zhao’s model is that the instances with different distribution and misaligned annotation guideline are ﬁltered out in data selection procedure. Different from these parameter-sharing architectures, Lee et al. [170] applied transfer learning in\n",
      "\n",
      "NER by training a model on source task and using the trained model on target task for ﬁne-tuning. Recently, Lin and Lu [171] also proposed a ﬁne-tuning approach for NER by introducing three neural adaptation layers: word adapta ion layer, sentence adaptation layer, and output adaptation layer. Beryozkin et al. [178] proposed a tag-hierarchy model for heterogeneous tag-sets NER setting, where the hierarchy is used during inference to map ﬁne-grained tags onto a target tag-set. In addition, some studies [164], [179], [180](explored transfer learning in biomedical NER to reduce the) amount of required labeled data.\n",
      "\n",
      "Deep Active Learning for NER\n",
      "\n",
      "The key idea behind active learning is that a machine learning algorithm can perform better with substantially less data from training, if it is allowed to choose the data from which it learns [181]. Deep learning typically requires a large amount of training data which is costly to obtain. Thus,\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020 combining deep learning with active learning is expected to reduce data annotation effort. Training with active learning proceeds in multiple rounds. However, traditional active learning schemes are expensive for deep learning because after each round they require complete retraining of the classiﬁer with newly annotated samples. Because retraining from scratch is not practical for deep learning, Shen et al. [88] proposed to carry out incremental training for NER with each batch of new labels. They mix newly annotated samples with the existing ones, and update neural network weights for a small number of epochs, before querying for labels in a new round. Speciﬁcally, at the beginning of each round, the active learning algorithm chooses sentences to be annotated, to the predeﬁned budget. The model parameters are up ated by training on the augmented dataset, after receiving chose annotations. The active learning algorithm adopts uncertainty sampling strategy [182] in selecting sentences to be annotated. Experimental results show that active learning algorithms achieve 99% performance of the best deep learning model trained on full data using only 24.9% of the training data on the English dataset and 30.1% on\n",
      "\n",
      "Chinese dataset. Moreover, 12.0% and 16.9% of training data were enough for deep active learning model to outperform shallow models learned on full training data [183].\n",
      "\n",
      "Deep Reinforcement Learning for NER\n",
      "\n",
      "Reinforcement learning (RL) is a branch of machine learning inspired by behaviorist psychology, which is concerned with how software agents take actions in an environment so as to maximize some cumulative rewards [184], [185]. The idea is that an agent will learn from the environment by interacting with it and receiving rewards for performing actions. Speciﬁcally, the RL problem can be formulated as follows [186]: the environment is modeled as a stochastic outputs (observations and rewards to the agent). It consists of three key components: (i) state transition function, (ii) observation (i.e., output) function, and (iii) reward function. The agent is also modeled as a stochastic ﬁnite state machine with inputs (observations/rewards from the environment) and outputs (actions to the environment). It consists of two components: (i) state transition function, and (ii) pol cy/output function. The ultimate goal of an agent is to learn a good state-update function and policy by attempting to maximize the cumulative rewards. Narasimhan et al. [187] modeled the task of informa ion extraction as a Markov decision process (MDP), which dynamically incorporates entity predictions and provides of issuing search queries, extraction from new sources, and reconciliation of extracted values, and the process repeats until sufﬁcient evidence is obtained. In order to learn a good policy for an agent, they utilize a deep Q-network [188](as a function approximator, in which the state-action value) function (i.e., Q-function) is approximated by using a deep neural network. Recently, Yang et al. [189] utilized the data generated by distant supervision to perform new type named entity recognition in new domains. The instance selector is based on reinforcement learning and obtains the feedback reward from the NE tagger, aiming at choosing positive sentences to reduce the effect of noisy annotation.\n",
      "\n",
      "Deep Adversarial Learning for NER\n",
      "\n",
      "Adversarial learning [190] is the process of explicitly train ng a model on adversarial examples. The purpose is to make the model more robust to attack or to reduce its test er or on clean inputs. Adversarial networks learn to generate from a training distribution through a 2-player game: one network generates candidates (generative network) and the other evaluates them (discriminative network). Typically, the generative network learns to map from a latent space to a particular data distribution of interest, while the discrimi ative network discriminates between candidates generated by the generator and instances from the real-world data distribution [191]. For NER, adversarial examples are often produced in two ways. Some studies [192]–[194] considered the instances in a source domain as adversarial examples for a target domain, and vice versa. For example, Li et al. [193] and\n",
      "\n",
      "Cao et al. [194] both incorporated adversarial examples from other domains to encourage domain-invariant features for cross-domain NER. Another option is to prepare an adversarial sample by adding an original sample with a perturbation. For example, dual adversarial transfer net ork (DATNet), proposed in [195], aims to deal with the problem of low-resource NER. An adversarial sample is produced by adding original sample with a perturbation bounded by a small norm ǫ to maximize the loss function as follows: ηx = arg\n",
      "\n",
      "model parameters set, ǫ can be determined on validation set. An adversarial example is constructed by xadv = x + ηx. The classiﬁer is trained on the mixture of original and adversarial examples to improve generalization.\n",
      "\n",
      "Neural Attention for NER\n",
      "\n",
      "The attention mechanism is loosely based on the visual attention mechanism found in human [196]. For example, people usually focus on a certain region of an image with with “low resolution”. Neural attention mechanism allows neural networks have the ability to focus on a subset of its inputs. By applying attention mechanism, a NER model could capture the most informative elements in the inputs. In particular, the Transformer architecture reviewed in Sec ion 3.3.5 relies entirely on attention mechanism to draw global dependencies between input and output. There are many other ways of applying attention mech nism in NER tasks. Rei et al. [105] applied an attention mechanism to dynamically decide how much information to use from a character- or word-level component in an end o-end NER model. Zukov-Gregoric et al. [197] explored the self-attention mechanism in NER, where the weights are de endent on a single sequence (rather than on the relation be ween two sequences). Xu et al. [198] proposed an attention ased neural NER architecture to leverage document-level global information. In particular, the document-level in ormation is obtained from document represented by pre rained bidirectional language model with neural attention. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "Zhang et al. [199] used an adaptive co-attention network for NER in tweets. This adaptive co-attention network is a multi-modal model using co-attention process. Co-attention includes visual attention and textual attention to capture the semantic interaction between different modalities.\n",
      "\n",
      "CHALLENGES AND FUTURE DIRECTIONS\n",
      "\n",
      "Discussed in Section 3.5, the choices tag decoders do not vary as much as the choices of input representations and context encoders. From Google Word2vec to the more recent\n",
      "\n",
      "BERT model, DL-based NER beneﬁts signiﬁcantly from the advances made in pre-trained embeddings in model ng languages. Without the need of complicated feature ngineering, we now have the opportunity to re-look the\n",
      "\n",
      "NER task for its challenges and potential future directions.\n",
      "\n",
      "Challenges\n",
      "\n",
      "Data Annotation. Supervised NER systems, including DL ased NER, require big annotated data in training. However, data annotation remains time consuming and expensive. It is a big challenge for many resource-poor languages and speciﬁc domains as domain experts are needed to perform annotation tasks. Quality and consistency of the annotation are both major concerns because of the language ambiguity. For instance, a same named entity may be annotated with different types. As an example, “Baltimore” in the sentence “Baltimore de eated the Yankees”, is labeled as Location in MUC-7 and\n",
      "\n",
      "Organization in CoNLL03. Both “Empire State” and “Empire\n",
      "\n",
      "State Building”, is labeled as Location in CoNLL03 and ACE datasets, causing confusion in entity boundaries. Because of the inconsistency in data annotation, model trained on one dataset may not work well on another even if the documents in the two datasets are from the same domain. To make data annotation even more complicated, Katiyar and Cardie [122] reported that nested entities are fairly common: 17% of the entities in the GENIA corpus are embedded within another entity; in the ACE corpora, 30% of sentences contain nested entities. There is a need to develop common annotation schemes to be applicable to both nested entities and ﬁne-grained entities, where one named entity may be assigned multiple types. Informal Text and Unseen Entities. Listed in Table 3, decent results are reported on datasets with formal documents (e.g., news articles). However, on user-generated text e.g., WUT 7 dataset, the best F-scores are slightly above 40%. NER on informal text (e.g., tweets, comments, user forums) is more challenging than on formal text due to the shortness and noisiness. Many user-generated texts are domain speciﬁc as well. In many application scenarios, a NER system has to deal with user-generated text such as customer support in e-commerce and banking. Another interesting dimension to evaluate the robust ess and effectiveness of NER system is its capability of identifying unusual, previously-unseen entities in the con ext of emerging discussions. There exists a shared task2 for this direction of research on WUT-17 dataset [200].\n",
      "\n",
      "2. https://noisy-text.github.io/2017/emerging-rare-entities.html\n",
      "\n",
      "Future Directions\n",
      "\n",
      "With the advances in modeling languages and demand in real-world applications, we expect NER to receive more attention from researchers. On the other hand, NER is in general considered as a pre-processing component to downstream applications. That means a particular NER task is deﬁned by the requirement of downstream application, e.g., the types of named entities and whether there is a need to detect nested entities [201]. Based on the studies in this survey, we list the following directions for further exploration in NER research. Fine-grained NER and Boundary Detection. While many existing studies [19], [97], [109] focused on coarse-grained\n",
      "\n",
      "NER in general domain, we expect more research on ﬁne rained NER in domain-speciﬁc areas to support various real word applications [202]. The challenges in ﬁne-grained\n",
      "\n",
      "NER are the signiﬁcant increase in NE types and the com lication introduced by allowing a named entity to have multiple NE types. This calls for a re-visit of the common\n",
      "\n",
      "NER approaches where the entity boundaries and the types are detected simultaneously e.g., by using B- I- E- S-(entity type) and O as the decoding tags. It is worth considering to deﬁne named entity boundary detection as a dedicated task to detect NE boundaries while ignoring the NE types. The decoupling of boundary detection and NE type classiﬁca ion enables common and robust solutions for boundary detection that can be shared across different domains, and dedicated domain-speciﬁc approaches for NE type classiﬁ ation. Correct entity boundaries also effectively alleviate error propagation in entity linking to knowledge bases. There has been some studies [95], [203] which consider entity boundary detection as an intermediate step (i.e., a subtask) in NER. To the best of our knowledge, no existing work separately focuses on entity boundary detection to provide a robust recognizer. We expect a breakout in this research direction in the future. Joint NER and Entity Linking. Entity linking (EL) [204], also referred to as named entity normalization or disam iguation, aims at assigning a unique identity to entities mentioned in text with reference to a knowledge base, e.g.,\n",
      "\n",
      "Wikipedia in general domain and the Uniﬁed Medical Lan uage System (UMLS) in biomedical domain. Most existing works individually solve NER and EL as two separate tasks in a pipeline setting. We consider that the semantics carried by the successfully linked entities (e.g., through the related entities in the knowledge base) are signiﬁcantly enriched\n",
      "\n",
      "[66], [205]. That is, linked entities contributes to the success ul detection of entity boundaries and correct classiﬁcation of entity types. It is worth exploring approaches for jointly performing NER and EL, or even entity boundary detection, entity type classiﬁcation, and entity linking, so that each subtask beneﬁts from the partial output by other subtasks, and alleviate error propagations that are unavoidable in pipeline settings. DL-based NER on Informal Text with Auxiliary Resource. As discussed in Section 5.1, performance of DL-based NER on informal text or user-generated content remains low. This calls for more research in this area. In particular, we note that the performance of NER beneﬁts signiﬁcantly from\n",
      "\n",
      "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020 the availability of auxiliary resources [206]–[208], e.g., a dictionary of location names in user language. While Table 3 does not provide strong evidence of involving gazetteer as additional features leads to performance increase to NER in general domain, we consider auxiliary resources are often necessary to better understand user-generated content. The question is how to obtain matching auxiliary resources for a\n",
      "\n",
      "NER task on user-generated content or domain-speciﬁc text, and how to effectively incorporate the auxiliary resources in\n",
      "\n",
      "DL-based NER. Scalability of DL-based NER. Making neural NER models more scalable is still a challenge. Moreover, there is still a need for solutions on optimizing exponential growth of parameters when the size of data grows [209]. Some\n",
      "\n",
      "DL-based NER models have achieved good performance with the cost of massive computing power. For exam le, the ELMo representation represents each word with a\n",
      "\n",
      "3 × 1024-dimensional vector, and the model was trained for\n",
      "\n",
      "5 weeks on 32 GPUs [107]. Google BERT representations were trained on 64 cloud TPUs. However, end users are not able to ﬁne-tune these models if they have no access to powerful computing resources. Developing approaches to balancing model complexity and scalability will be a promising direction. On the other hand, model compression and pruning techniques are also options to reduce the space and computation time required for model learning. Deep Transfer Learning for NER. Many entity-focused applications resort to off-the-shelf NER systems to recognize named entities. However, model trained on one dataset may not work well on other texts due to the differences in characteristics of languages as well as the differences in annotations. Although there are some studies of applying deep transfer learning to NER (see Section 4.2), this problem has not been fully explored. More future efforts should be dedicated on how to effectively transfer knowledge from one domain to another by exploring the following research problems: (a) developing a robust recognizer, which is able to work well across different domains; (b) exploring zero hot, one-shot and few-shot learning in NER tasks; (c) providing solutions to address domain mismatch, and label mismatch in cross-domain settings. An Easy-to-use Toolkit for DL-based NER. Recently, Röder et al. [210] developed GERBIL, which provides researchers, end users and developers with easy-to-use interfaces for benchmarking entity annotation tools with the aim of en uring repeatable and archiveable experiments. However, it does not involve recent DL-based techniques. Ott [211] pre ented FAIRSEQ, a fast, extensible toolkit for sequence mod ling, especially for machine translation and text stigma ization. Dernoncourt et al. [212] implemented a frame ork, named NeuroNER, which only relies on a variant of recurrent neural network. In recent years, many deep learning frameworks (e.g., TensorFlow, PyTorch, and Keras) have been designed to offer building blocks for designing, training and validating deep neural networks, through a high level programming interface.3 In order to re-implement the architectures in Table 3, developers may write codes from scratch with existing deep learning frameworks. We\n",
      "\n",
      "3. https://developer.nvidia.com/deep-learning-frameworks envision that an easy-to-use NER toolkit can guide de elopers to complete it with some standardized modules: data-processing, input representation, context encoder, tag decoder, and effectiveness measure. We believe that experts and non-experts can both beneﬁt from such toolkits.\n",
      "\n",
      "CONCLUSION\n",
      "\n",
      "This survey aims to review recent studies on deep learning ased NER solutions to help new researchers building a comprehensive understanding of this ﬁeld. We include in this survey the background of the NER research, a brief of traditional approaches, current state-of-the-arts, and chal enges and future research directions. First, we consolidate available NER resources, including tagged NER corpora and off-the-shelf NER systems, with focus on NER in general domain and NER in English. We present these resources in a tabular form and provide links to them for easy ac ess. Second, we introduce preliminaries such as deﬁnition of NER task, evaluation metrics, traditional approaches to\n",
      "\n",
      "NER, and basic concepts in deep learning. Third, we review the literature based on varying models of deep learning and map these studies according to a new taxonomy. We further survey the most representative methods for recent applied deep learning techniques in new problem settings and applications. Finally, we summarize the applications of\n",
      "\n",
      "NER and present readers with challenges in NER and future directions. We hope that this survey can provide a good reference when designing DL-based NER models. REFERENCES\n",
      "\n",
      "[1](D. Nadeau and S. Sekine, “A survey of named entity recognition) and classiﬁcation,” Lingvist. Investig., vol. 30, no. 1, pp. 3–26,\n",
      "\n",
      "2007. [2](Z. Zhang,\n",
      "\n",
      "X. Han, Z. Liu,\n",
      "\n",
      "X. Jiang,\n",
      "\n",
      "M. Sun, and Q. Liu, “ERNIE:) enhanced language representation with informative entities,” in\n",
      "\n",
      "ACL, 2019, pp. 1441–1451.\n",
      "\n",
      "[3](P. Cheng and K. Erk, “Attending to entities for better text under tanding,” arXiv preprint arXiv:1911.04361, 2019.)\n",
      "\n",
      "[4](J. Guo, G. Xu,\n",
      "\n",
      "X. Cheng, and H. Li, “Named entity recognition in) query,” in SIGIR, 2009, pp. 267–274.\n",
      "\n",
      "[5](D. Petkova and W. B. Croft, “Proximity-based document repre entation for named entity retrieval,” in CIKM, 2007, pp. 731–740.)\n",
      "\n",
      "[6](C. Aone,\n",
      "\n",
      "M. E. Okurowski, and J. Gorlinsky, “A trainable sum arizer with knowledge acquired from robust nlp techniques,”)\n",
      "\n",
      "Adv. Autom. Text Summ., vol. 71,\n",
      "\n",
      "1999. [7](D.\n",
      "\n",
      "M. Aliod,\n",
      "\n",
      "M. van Zaanen, and\n",
      "\n",
      "D. Smith, “Named entity) recognition for question answering,” in ALTA, 2006, pp. 51–58.\n",
      "\n",
      "[8](B. Babych and A. Hartley, “Improving machine translation qual ty with automatic named entity recognition,” in EAMT, 2003, pp.)\n",
      "\n",
      "[9](O. Etzioni,\n",
      "\n",
      "M. Cafarella,\n",
      "\n",
      "D. Downey, A.-M. Popescu, T. Shaked,)\n",
      "\n",
      "S. Soderland,\n",
      "\n",
      "D. S. Weld, and A. Yates, “Unsupervised named ntity extraction from the web: An experimental study,” Artif. Intell., vol. 165, no. 1, pp. 91–134,\n",
      "\n",
      "2005. [10](R. Grishman)\n",
      "\n",
      "B. Sundheim, understanding\n",
      "\n",
      "conference-6: A brief history,” in COLING, vol. 1,\n",
      "\n",
      "1996. [11](E. F. Tjong Kim Sang and F. De Meulder, “Introduction to) the conll-2003 shared task: Language-independent named entity recognition,” in NAACL-HLT, 2003, pp. 142–147.\n",
      "\n",
      "[12](G. R. Doddington, A. Mitchell,\n",
      "\n",
      "M. A. Przybocki,\n",
      "\n",
      "L. A. Ramshaw,)\n",
      "\n",
      "S. Strassel, and R.\n",
      "\n",
      "M. Weischedel, “The automatic content extrac ion (ace) program-tasks, data, and evaluation.” in LREC, vol. 2,\n",
      "\n",
      "2004, p.\n",
      "\n",
      "1. [13](G. Demartini, T. Iofciu, and A. P. De Vries, “Overview of the inex)\n",
      "\n",
      "2009 entity ranking track,” in INEX, 2009, pp. 254–264.\n",
      "\n",
      "[14](K. Balog, P. Serdyukov, and A. P. De Vries, “Overview of the trec)\n",
      "\n",
      "2010 entity track,” in TREC,\n",
      "\n",
      "2010. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "[15](G. Petasis, A. Cucchiarelli, P. Velardi, G. Paliouras,\n",
      "\n",
      "V. Karkaletsis,)\n",
      "\n",
      "C. D. Spyropoulos, “Automatic adaptation of proper noun dictionaries through cooperation of machine learning and prob bilistic methods,” in SIGIR, 2000, pp. 128–135.\n",
      "\n",
      "[16](S. A. Kripke, “Naming and necessity,” in Semantics of natural) language. Springer, 1972, pp. 253–355.\n",
      "\n",
      "[17](R. Collobert, J. Weston,\n",
      "\n",
      "L. Bottou,\n",
      "\n",
      "M. Karlen, K. Kavukcuoglu,) and P. Kuksa, “Natural language processing (almost) from scratch,” J. Mach. Learn. Res., vol. 12, no. Aug, pp. 2493–2537,\n",
      "\n",
      "2011. [18](Z. Huang, W. Xu, and K. Yu, “Bidirectional lstm-crf models for) sequence tagging,” arXiv preprint arXiv:1508.01991,\n",
      "\n",
      "2015. [19](G. Lample,\n",
      "\n",
      "M. Ballesteros, S. Subramanian, K. Kawakami, and)\n",
      "\n",
      "C. Dyer, “Neural architectures for named entity recognition,” in\n",
      "\n",
      "NAACL, 2016, pp. 260–270.\n",
      "\n",
      "[20](J. P. Chiu and E. Nichols, “Named entity recognition with bidi ectional lstm-cnns,” Trans. Assoc. Comput. Linguist., pp. 357–370,)\n",
      "\n",
      "2016. [21](M. E. Peters, W. Ammar,\n",
      "\n",
      "C. Bhagavatula, and R. Power, “Semi upervised sequence tagging with bidirectional language mod ls,” in ACL, 2017, pp. 1756–1765.)\n",
      "\n",
      "[22](M. Marrero, J. Urbano, S. Sánchez-Cuadrado, J. Morato, and J.\n",
      "\n",
      "M. Gómez-Berbís, “Named entity recognition: fallacies, challenges) and opportunities,” Comput. Stand. Interfaces, vol. 35, no. 5, pp.\n",
      "\n",
      "482–489,\n",
      "\n",
      "2013. [23](M.\n",
      "\n",
      "L. Patawar and\n",
      "\n",
      "M. Potey, “Approaches to named entity) recognition: a survey,” Int. J. Innov. Res. Comput. Commun. Eng., vol. 3, no. 12, pp. 12 201–12 208,\n",
      "\n",
      "2015. [24](C. J. Saju and A. Shaja, “A survey on efﬁcient extraction of) named entities from new domains using big data analytics,” in\n",
      "\n",
      "ICRTCCM, 2017, pp. 170–175.\n",
      "\n",
      "[25](X. Dai, “Recognizing complex entity mentions: A review and) future directions,” in ACL, 2018, pp. 37–44.\n",
      "\n",
      "[26](V. Yadav and S. Bethard, “A survey on recent advances in named) entity recognition from deep learning models,” in COLING, 2018, pp. 2145–2158.\n",
      "\n",
      "[27](A. Goyal,\n",
      "\n",
      "V. Gupta, and\n",
      "\n",
      "M. Kumar, “Recent named entity) recognition and classiﬁcation techniques: A systematic review,”\n",
      "\n",
      "Comput. Sci. Rev., vol. 29, pp. 21–43,\n",
      "\n",
      "2018. [28](R. Sharnagat, “Named entity recognition: A literature survey,”)\n",
      "\n",
      "Center For Indian Language Technology,\n",
      "\n",
      "2014. [29](X. Ling and\n",
      "\n",
      "D. S. Weld, “Fine-grained entity recognition.” in)\n",
      "\n",
      "AAAI, vol. 12, 2012, pp. 94–100.\n",
      "\n",
      "[30](X. Ren, W. He,\n",
      "\n",
      "L. Huang, H. Ji, and J. Han, “Afet:)\n",
      "\n",
      "Automatic ﬁne-grained entity typing by hierarchical partial-label embedding,” in EMNLP, 2016, pp. 1369–1378.\n",
      "\n",
      "[31](A. Abhishek, A. Anand, and A. Awekar, “Fine-grained entity) type classiﬁcation by jointly learning representations and label embeddings,” in EACL, 2017, pp. 797–807.\n",
      "\n",
      "[32](A. Lal, A. Tomer, and\n",
      "\n",
      "C. R. Chowdary, “Sane: System for ﬁne) grained named entity typing on textual data,” in WWW, 2017, pp. 227–230.\n",
      "\n",
      "d. Corro, A. Abujabal, R. Gemulla, and G. Weikum, “Finet:)\n",
      "\n",
      "Context-aware ﬁne-grained named entity typing,” in EMNLP,\n",
      "\n",
      "2015, pp. 868–878.\n",
      "\n",
      "[34](K. Balog, Entity-Oriented Search. Springer, 2018.)\n",
      "\n",
      "[35](H. Raviv, O. Kurland, and\n",
      "\n",
      "D. Carmel, “Document retrieval using) entity-based language models,” in SIGIR, 2016, pp. 65–74.\n",
      "\n",
      "[36](P. Boldi, F. Bonchi,\n",
      "\n",
      "C. Castillo,\n",
      "\n",
      "D. Donato, A. Gionis, and S. Vigna,) pp. 609–618.\n",
      "\n",
      "[37](F. Cai,\n",
      "\n",
      "M. De Rijke et al., “A survey of query auto completion in) information retrieval,” Found. Trends R⃝ in Inf. Retr., vol. 10, no. 4, pp. 273–363,\n",
      "\n",
      "2016. [38](Z. Bar-Yossef and N. Kraus, “Context-sensitive query auto ompletion,” in WWW, 2011, pp. 107–116.)\n",
      "\n",
      "[39](G. Saldanha, O. Biran, K. McKeown, and A. Gliozzo, “An entity ocused approach to generating company descriptions,” in ACL,) vol. 2, 2016, pp. 243–248.\n",
      "\n",
      "[40](F. Hasibi, K. Balog, and S. E. Bratsberg, “Dynamic factual sum aries for entity cards,” in SIGIR, 2017, pp. 773–782.)\n",
      "\n",
      "[41](S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and Y. Zhang,) coreference in ontonotes,” in EMNLP, 2012, pp. 1–40.\n",
      "\n",
      "[42](C. Dogan, A. Dutra, A. Gara, A. Gemma,\n",
      "\n",
      "M. Sigamani, and)\n",
      "\n",
      "E. Walters, “Fine-grained named entity recognition using elmo and wikidata,” CoRR, vol. abs/1904.10503,\n",
      "\n",
      "2019. [43](S. Sekine and\n",
      "\n",
      "C. Nobata, “Deﬁnition, dictionaries and tagger for) extended named entity hierarchy.” in LREC, 2004, pp. 1977–1980.\n",
      "\n",
      "[44](S. Zhang and N. Elhadad, “Unsupervised biomedical named en ity recognition: Experiments with clinical and biological texts,”)\n",
      "\n",
      "J. Biomed. Inform., vol. 46, no. 6, pp. 1088–1098,\n",
      "\n",
      "2013. [45](J.-H. Kim and P.\n",
      "\n",
      "C. Woodland, “A rule-based named entity) recognition system for speech input,” in ICSLP,\n",
      "\n",
      "2000. [46](D. Hanisch, K. Fundel, H.-T. Mevissen, R. Zimmer, and J. Fluck,)\n",
      "\n",
      "Bioinform., vol. 6, no. 1, p. S14,\n",
      "\n",
      "2005. [47](A. P. Quimbaya, A. S. Múnera, R. A. G. Rivera, J.\n",
      "\n",
      "C. D. Rodríguez,)\n",
      "\n",
      "M. M. Velandia, A. A. G. Peña, and\n",
      "\n",
      "C. Labbé, “Named entity recognition over electronic health records through a combined dictionary-based approach,” Procedia Comput. Sci., vol. 100, pp.\n",
      "\n",
      "2016. [48](K. Humphreys, R. Gaizauskas, S. Azzam,\n",
      "\n",
      "C. Huyck, B. Mitchell,)\n",
      "\n",
      "H. Cunningham, and Y. Wilks, “University of shefﬁeld: Descrip ion of the lasie-ii system as used for muc-7,” in MUC-7,\n",
      "\n",
      "1998. [49](G. Krupka and K. IsoQuest, “Description of the nerowl extractor) system as used for muc-7,” in MUC-7, 2005, pp. 21–28.\n",
      "\n",
      "[50](W. J. Black, F. Rinaldi, and\n",
      "\n",
      "D. Mowatt, “Facile: Description of the) ne system used for muc-7,” in MUC-7,\n",
      "\n",
      "1998. [51](C. Aone,\n",
      "\n",
      "L. Halverson, T. Hampton, and\n",
      "\n",
      "M. Ramos-Santacruz,)\n",
      "\n",
      "1998. [52](D. E. Appelt, J. R. Hobbs, J. Bear,\n",
      "\n",
      "D. Israel,\n",
      "\n",
      "M. Kameyama,)\n",
      "\n",
      "D. Martin, K. Myers, and\n",
      "\n",
      "M. Tyson, “Sri international fastus system: Muc-6 test results and analysis,” in MUC-6, 1995, pp.\n",
      "\n",
      "237–248.\n",
      "\n",
      "[53](A. Mikheev,\n",
      "\n",
      "M. Moens, and\n",
      "\n",
      "C. Grover, “Named entity recognition) without gazetteers,” in EACL, 1999, pp. 1–8.\n",
      "\n",
      "[54](M. Collins and Y. Singer, “Unsupervised models for named entity) classiﬁcation,” in EMNLP, 1999, pp. 100–110.\n",
      "\n",
      "[55](D. Nadeau, P.\n",
      "\n",
      "D. Turney, and S. Matwin, “Unsupervised named ntity recognition: Generating gazetteers and resolving ambigu ty,” in CSCSI, 2006, pp. 266–277.)\n",
      "\n",
      "[56](S. Sekine and E. Ranchhod, Named entities: recognition, classiﬁca ion and use. John Benjamins Publishing, 2009, vol. 19.)\n",
      "\n",
      "[57](G. Zhou and J. Su, “Named entity recognition using an hmm ased chunk tagger,” in ACL, 2002, pp. 473–480.)\n",
      "\n",
      "[58](B. Settles, “Biomedical named entity recognition using condi ional random ﬁelds and rich feature sets,” in ACL, 2004, pp.)\n",
      "\n",
      "104–107.\n",
      "\n",
      "[59](W. Liao and S. Veeramachaneni, “A simple semi-supervised) algorithm for named entity recognition,” in NAACL-HLT, 2009, pp. 58–65.\n",
      "\n",
      "[60](A. Mikheev, “A knowledge-free method for capitalized word) disambiguation,” in ACL, 1999, pp. 159–166.\n",
      "\n",
      "[61](J. Kazama and K. Torisawa, “Exploiting wikipedia as external) knowledge for named entity recognition,” in EMNLP-CoNLL,\n",
      "\n",
      "2007. [62](A. Toral and R. Munoz, “A proposal to automatically build) and maintain gazetteers for named entity recognition by using wikipedia,” in Workshop on NEW TEXT Wikis and blogs and other dynamic text sources,\n",
      "\n",
      "2006. [63](J. Hoffart,\n",
      "\n",
      "M. A. Yosef,\n",
      "\n",
      "I. Bordino, H. Fürstenau,\n",
      "\n",
      "M. Pinkal,)\n",
      "\n",
      "M. Spaniol, B. Taneva, S. Thater, and G. Weikum, “Robust disam iguation of named entities in text,” in EMNLP, 2011, pp. 782–\n",
      "\n",
      "792. [64](Y. Ravin and N. Wacholder, Extracting names from natural-language) text. IBM Research Report RC 2033,\n",
      "\n",
      "1997. [65](J. Zhu,\n",
      "\n",
      "V. Uren, and E. Motta, “Espotter: Adaptive named entity) recognition for web browsing,” in WM. Springer, 2005, pp. 518–\n",
      "\n",
      "529. [66](Z. Ji, A. Sun, G. Cong, and J. Han, “Joint recognition and linking) of ﬁne-grained locations from tweets,” in WWW, 2016, pp. 1271–\n",
      "\n",
      "1281. [67](V. Krishnan and\n",
      "\n",
      "C. D. Manning, “An effective two-stage model) for exploiting non-local dependencies in named entity recogni ion,” in ACL, 2006, pp. 1121–1128.\n",
      "\n",
      "[68](D. Campos, S. Matos, and J.\n",
      "\n",
      "L. Oliveira, “Biomedical named) entity recognition: a survey of machine-learning tools,” in Theory\n",
      "\n",
      "Appl. Adv. Text Min.,\n",
      "\n",
      "2012. [69](S. R. Eddy, “Hidden markov models,” Curr. Opin. Struct. Biol.,) vol. 6, no. 3, pp. 361–365,\n",
      "\n",
      "1996. [70](J. R. Quinlan, “Induction of decision trees,” Mach. Learn., vol. 1,) no. 1, pp. 81–106,\n",
      "\n",
      "1986. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "[71](J. N. Kapur, Maximum-entropy models in science and engineering. John Wiley & Sons, 1989.)\n",
      "\n",
      "[72](M. A. Hearst, S. T. Dumais, E. Osuna, J. Platt, and B. Scholkopf,) no. 4, pp. 18–28,\n",
      "\n",
      "1998. [73](J.\n",
      "\n",
      "D. Lafferty, A. McCallum, and F.\n",
      "\n",
      "C. N. Pereira, “Conditional) random ﬁelds: Probabilistic models for segmenting and labeling sequence data,” pp. 282–289,\n",
      "\n",
      "2001. [74](D.\n",
      "\n",
      "M. Bikel, S. Miller, R. Schwartz, and R. Weischedel, “Nymble:) a high-performance learning name-ﬁnder,” in ANLC, 1997, pp.\n",
      "\n",
      "194–201.\n",
      "\n",
      "M. Bikel, R. Schwartz, and R.\n",
      "\n",
      "M. Weischedel, “An algorithm) that learns what’s in a name,” Mach. Learn., vol. 34, no. 1-3, pp.\n",
      "\n",
      "211–231,\n",
      "\n",
      "1999. [76](G. Szarvas, R. Farkas, and A. Kocsor, “A multilingual named) entity recognition system using boosting and c4. 5 decision tree learning algorithms,” in DS. Springer, 2006, pp. 267–278.\n",
      "\n",
      "[77](A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman, “Nyu:)\n",
      "\n",
      "Description of the mene named entity system as used in muc-7,” in MUC-7,\n",
      "\n",
      "1998. [78](O. Bender, F. J. Och, and H. Ney, “Maximum entropy models for) named entity recognition,” in HLT-NAACL, 2003, pp. 148–151.\n",
      "\n",
      "L. Chieu and H. T. Ng, “Named entity recognition: a max mum entropy approach using global information,” in CoNLL,)\n",
      "\n",
      "2002, pp. 1–7.\n",
      "\n",
      "[80](J. R. Curran and S. Clark, “Language independent ner using a) maximum entropy tagger,” in HLT-NAACL, 2003, pp. 164–167.\n",
      "\n",
      "[81](P. McNamee)\n",
      "\n",
      "J. Mayﬁeld, extraction\n",
      "\n",
      "language-speciﬁc resources,” in CoNLL, 2002, pp. 1–4.\n",
      "\n",
      "[82](A. McCallum and W. Li, “Early results for named entity recogni ion with conditional random ﬁelds, feature induction and web nhanced lexicons,” in HLT-NAACL, 2003, pp. 188–191.)\n",
      "\n",
      "[83](S. Liu, Y. Sun, B. Li, W. Wang, and\n",
      "\n",
      "X. Zhao, “Hamner:)\n",
      "\n",
      "Headword ampliﬁed multi-span distantly supervised method for domain speciﬁc named entity recognition,” arXiv preprint arXiv:1912.01731,\n",
      "\n",
      "2019. [84](A. Ritter, S. Clark, O. Etzioni et al., “Named entity recognition in) tweets: an experimental study,” in EMNLP, 2011, pp. 1524–1534.\n",
      "\n",
      "[85](X. Liu, S. Zhang, F. Wei, and\n",
      "\n",
      "M. Zhou, “Recognizing named) entities in tweets,” in ACL, 2011, pp. 359–367.\n",
      "\n",
      "[86](T. Rocktäschel,\n",
      "\n",
      "M. Weidlich, and U. Leser, “Chemspot: a hybrid) system for chemical named entity recognition,” Bioinformatics, vol. 28, no. 12, pp. 1633–1640,\n",
      "\n",
      "2012. [87](Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.)\n",
      "\n",
      "521, no. 7553, p. 436,\n",
      "\n",
      "2015. [88](Y. Shen, H. Yun, Z.\n",
      "\n",
      "C. Lipton, Y. Kronrod, and A. Anandkumar,)\n",
      "\n",
      "2017. [89](T. H. Nguyen, A. Sil, G. Dinu, and R. Florian, “Toward men ion detection robustness with recurrent neural networks,” arXiv) preprint arXiv:1602.07749,\n",
      "\n",
      "2016. [90](S. Zheng, F. Wang, H. Bao, Y. Hao, P. Zhou, and B. Xu, “Joint) extraction of entities and relations based on a novel tagging scheme,” in ACL, 2017, pp. 1227–1236.\n",
      "\n",
      "[91](E. Strubell, P. Verga,\n",
      "\n",
      "D. Belanger, and A. McCallum, “Fast and) accurate entity recognition with iterated dilated convolutions,” in ACL, 2017, pp. 2670–2680.\n",
      "\n",
      "[92](T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estima ion of word representations in vector space,” in ICLR, 2013.)\n",
      "\n",
      "[93](J. Yang, S. Liang, and Y. Zhang, “Design challenges and mis onceptions in neural sequence labeling,” in COLING, 2018, pp.)\n",
      "\n",
      "3879–3889.\n",
      "\n",
      "[94](L. Yao, H. Liu, Y. Liu,\n",
      "\n",
      "X. Li, and\n",
      "\n",
      "M. W. Anwar, “Biomedical) named entity recognition based on deep neutral network,” Int. J. Hybrid Inf. Technol., vol. 8, no. 8, pp. 279–288,\n",
      "\n",
      "2015. [95](F. Zhai, S. Potdar, B. Xiang, and B. Zhou, “Neural models for) sequence chunking.” in AAAI, 2017, pp. 3365–3371.\n",
      "\n",
      "[96](P. Zhou, S. Zheng, J. Xu, Z. Qi, H. Bao, and B. Xu, “Joint) extraction of multiple relations and entities by using a hybrid neural network,” in CCL-NLP-NABD. Springer, 2017, pp. 135–\n",
      "\n",
      "146. [97](X. Ma and E. Hovy, “End-to-end sequence labeling via bi irectional lstm-cnns-crf,” in ACL, 2016, pp. 1064–1074.)\n",
      "\n",
      "[98](P.-H. Li, R.-P. Dong, Y.-S. Wang, J.-C. Chou, and W.-Y. Ma,) with bidirectional recursive neural networks,” in EMNLP, 2017, pp. 2664–2669.\n",
      "\n",
      "[99](C. Wang, K. Cho, and\n",
      "\n",
      "D. Kiela, “Code-switched named entity) recognition with embedding attention,” in CALCS, 2018, pp. 154–\n",
      "\n",
      "158. [100] O. Kuru, O. A. Can, and\n",
      "\n",
      "D. Yuret, “Charner: Character-level named entity recognition,” in COLING, 2016, pp. 911–921.\n",
      "\n",
      "[101] Q. Tran, A. MacKinlay, and A. J. Yepes, “Named entity recog ition with stack residual lstm and trainable bias decoding,” in\n",
      "\n",
      "IJCNLP, 2017, pp. 566–575.\n",
      "\n",
      "[102] J. Yang, Y. Zhang, and F. Dong, “Neural reranking for named entity recognition,” in RANLP, 2017, pp. 784–792.\n",
      "\n",
      "M. E. Peters,\n",
      "\n",
      "M. Neumann,\n",
      "\n",
      "M. Iyyer,\n",
      "\n",
      "M. Gardner,\n",
      "\n",
      "C. Clark, K. Lee,\n",
      "\n",
      "L. Zettlemoyer, “Deep contextualized word representations,” in NAACL-HLT, 2018, pp. 2227–2237.\n",
      "\n",
      "M. Gridach, “Character-level neural network for biomedical named entity recognition,” J. Biomed. Inform., vol. 70, pp. 85–91,\n",
      "\n",
      "2017. [105]\n",
      "\n",
      "M. Rei, G. K. Crichton, and S. Pyysalo, “Attending to characters in neural sequence labeling models,” in COLING, 2016, pp. 309–\n",
      "\n",
      "318. [106] Z. Yang,\n",
      "\n",
      "R. Salakhutdinov,\n",
      "\n",
      "W. Cohen, cross-lingual sequence tagging from scratch,” arXiv preprint arXiv:1603.06270,\n",
      "\n",
      "2016. [107] A. Akbik,\n",
      "\n",
      "D. Blythe, and R. Vollgraf, “Contextual string embed ings for sequence labeling,” in COLING, 2018, pp. 1638–1649.\n",
      "\n",
      "[108] T. Liu, J. Yao, and\n",
      "\n",
      "C. Lin, “Towards improving neural named entity recognition with gazetteers,” in ACL, 2019, pp. 5301–5307.\n",
      "\n",
      "[109] A. Ghaddar and P. Langlais, “Robust lexical features for im roved neural network named-entity recognition,” in COLING,\n",
      "\n",
      "2018, pp. 1896–1907.\n",
      "\n",
      "[110] Z. Jie and W. Lu, “Dependency-guided lstm-crf for named entity recognition,” in EMNLP, 2018, pp. 3860–3870.\n",
      "\n",
      "L. Neves,\n",
      "\n",
      "V. Carvalho, N. Zhang, and H. Ji, “Visual attention model for name tagging in multimodal social media,” in ACL, 2018, pp. 1990–1999.\n",
      "\n",
      "[112] Q. Wei, T. Chen, R. Xu, Y. He, and\n",
      "\n",
      "L. Gui, “Disease named entity recognition by combining conditional random ﬁelds and bidirectional recurrent neural networks,” Database, vol. 2016,\n",
      "\n",
      "2016. [113] B. Y. Lin, F. Xu, Z. Luo, and K. Zhu, “Multi-channel bilstm-crf model for emerging named entity recognition in social media,” in W-NUT, 2017, pp. 160–165.\n",
      "\n",
      "[114] G. Aguilar, S. Maharjan, A. P.\n",
      "\n",
      "L. Monroy, and T. Solorio, “A multi ask approach for named entity recognition in social media data,” in W-NUT, 2017, pp. 148–153.\n",
      "\n",
      "[115] P. Jansson and S. Liu, “Distributed representation, lda topic mod lling and deep learning for emerging named entity recognition from social media,” in W-NUT, 2017, pp. 154–159.\n",
      "\n",
      "M. Xu, H. Jiang, and S. Watcharawittayakul, “A local detection approach for named entity recognition and mention detection,” in ACL, vol. 1, 2017, pp. 1237–1247.\n",
      "\n",
      "[117] S. Zhang, H. Jiang,\n",
      "\n",
      "M. Xu, J. Hou, and\n",
      "\n",
      "L. Dai, “A ﬁxed ize encoding method for variable-length sequences with its application to neural network language models,” arXiv preprint arXiv:1505.01504,\n",
      "\n",
      "2015. [118] S. Moon,\n",
      "\n",
      "L. Neves, and\n",
      "\n",
      "V. Carvalho, “Multimodal named entity recognition for short social media posts,” in NAACL, 2018, pp.\n",
      "\n",
      "852–860.\n",
      "\n",
      "[119] J. Devlin,\n",
      "\n",
      "M. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” in NAACL-HLT, 2019, pp. 4171–4186.\n",
      "\n",
      "[120] Y. Wu,\n",
      "\n",
      "M. Jiang, J. Lei, and H. Xu, “Named entity recognition in chinese clinical text using deep neural network,” MEDINFO, pp.\n",
      "\n",
      "624–628,\n",
      "\n",
      "2015. [121] A. Z. Gregoric, Y. Bachrach, and S. Coope, “Named entity recog ition with parallel recurrent neural networks,” in ACL, vol. 2,\n",
      "\n",
      "2018, pp. 69–74.\n",
      "\n",
      "[122] A. Katiyar and\n",
      "\n",
      "C. Cardie, “Nested named entity recognition revisited,” in ACL, vol. 1, 2018, pp. 861–871.\n",
      "\n",
      "M. Miwa, and S. Ananiadou, “A neural layered model for nested named entity recognition,” in NAACL-HLT, vol. 1, 2018, pp. 1446–1459.\n",
      "\n",
      "M. Rei, “Semi-supervised multitask learning for sequence label ng,” in ACL, 2017, pp. 2121–2130.\n",
      "\n",
      "X. Ren, J. Shang, J. Peng, and J. Han, “Efﬁcient contex ualized representation: Language model pruning for sequence labeling,” in EMNLP, 2018, pp. 1215–1225. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "L. Liu, J. Shang, F. Xu,\n",
      "\n",
      "X. Ren, H. Gui, J. Peng, and J. Han, model,” in AAAI, 2017, pp. 5253–5260.\n",
      "\n",
      "L. Xiao, and Y. Zhang, “Cross-domain NER using cross omain language modeling,” in ACL, 2019, pp. 2464–2474.\n",
      "\n",
      "[128] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\n",
      "\n",
      "L. Jones, A. N. Gomez, Ł. Kaiser, and\n",
      "\n",
      "I. Polosukhin, “Attention is all you need,” in NIPS, 2017, pp. 5998–6008.\n",
      "\n",
      "[129] P. J. Liu,\n",
      "\n",
      "M. Saleh, E. Pot, B. Goodrich, R. Sepassi,\n",
      "\n",
      "L. Kaiser, and N. Shazeer, “Generating wikipedia by summarizing long sequences,” arXiv preprint arXiv:1801.10198,\n",
      "\n",
      "2018. [130] N. Kitaev and\n",
      "\n",
      "D. Klein, “Constituency parsing with a self ttentive encoder,” in ACL, 2018, pp. 2675–2685.\n",
      "\n",
      "[131] A. Radford, K. Narasimhan, T. Salimans, and\n",
      "\n",
      "I. Sutskever, “Im roving language understanding by generative pre-training,”\n",
      "\n",
      "Technical Report, OpenAI,\n",
      "\n",
      "2018. [132] A. Baevski, S. Edunov, Y. Liu,\n",
      "\n",
      "L. Zettlemoyer, and\n",
      "\n",
      "M. Auli, abs/1903.07785,\n",
      "\n",
      "2019. [133]\n",
      "\n",
      "C. Zhang, T. Yang, Y. Li, N. Du,\n",
      "\n",
      "X. Wu, W. Fan, F. Ma, and\n",
      "\n",
      "P. S. Yu, “Multi-grained named entity recognition,” in ACL, 2019, pp. 1430–1440.\n",
      "\n",
      "[134] Y. Luo,\n",
      "\n",
      "F. Xiao,\n",
      "\n",
      "H. Zhao, contextual zed representation for named entity recognition,” CoRR, vol. abs/1911.02257,\n",
      "\n",
      "2019. [135] Y. Liu, F. Meng, J. Zhang, J. Xu, Y. Chen, and J. Zhou, “GCDT: A global context enhanced deep transition architecture for sequence labeling,” in ACL, 2019, pp. 2431–2441.\n",
      "\n",
      "[136] Y. Jiang,\n",
      "\n",
      "C. Hu, T. Xiao,\n",
      "\n",
      "C. Zhang, and J. Zhu, “Improved differ ntiable architecture search for language modeling and named entity recognition,” in EMNLP, 2019, pp. 3576–3581.\n",
      "\n",
      "X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, and J. Li, “A uni ed MRC framework for named entity recognition,” CoRR, vol. abs/1910.11476,\n",
      "\n",
      "2019. [138]\n",
      "\n",
      "X. Sun, Y. Meng, J. Liang, F. Wu, and J. Li, “Dice loss for data-imbalanced NLP tasks,” CoRR, vol. abs/1911.02855,\n",
      "\n",
      "2019. [139]\n",
      "\n",
      "L. Cui and Y. Zhang, “Hierarchically-reﬁned label attention net ork for sequence labeling,” in EMNLP, 2019, pp. 4113–4126.\n",
      "\n",
      "[140] S. Tomori, T. Ninomiya, and S. Mori, “Domain speciﬁc named entity recognition referring to the real world by deep neural networks,” in ACL, vol. 2, 2016, pp. 236–242.\n",
      "\n",
      "[141] Y. Lin,\n",
      "\n",
      "L. Liu, H. Ji,\n",
      "\n",
      "D. Yu, and J. Han, “Reliability-aware dynamic feature composition for name tagging,” in ACL, 2019, pp. 165–\n",
      "\n",
      "174. [142] J. Zhuo, Y. Cao, J. Zhu, B. Zhang, and Z. Nie, “Segment-level se uence modeling using gated recursive semi-markov conditional random ﬁelds,” in ACL, vol. 1, 2016, pp. 1413–1423.\n",
      "\n",
      "[143] Z.-X. Ye and Z.-H. Ling, “Hybrid semi-markov crf for neural sequence labeling,” in ACL, 2018, pp. 235–240.\n",
      "\n",
      "[144] A. Vaswani, Y. Bisk, K. Sagae, and R. Musa, “Supertagging with lstms,” in NAACL-HLT, 2016, pp. 232–237.\n",
      "\n",
      "[145] O. Vinyals,\n",
      "\n",
      "M. Fortunato, and N. Jaitly, “Pointer networks,” in\n",
      "\n",
      "NIPS, 2015, pp. 2692–2700.\n",
      "\n",
      "[146] J. Li, A. Sun, and S. Joty, “Segbot: A generic neural text segmenta ion model with pointer network,” in IJCAI, 2018, pp. 4166–4172.\n",
      "\n",
      "[147] Q. Guo,\n",
      "\n",
      "X. Qiu, P. Liu, Y. Shao,\n",
      "\n",
      "X. Xue, and Z. Zhang, “Star ransformer,” in NAACL-HLT, 2019, pp. 1315–1325.\n",
      "\n",
      "[148] H. Yan, B. Deng,\n",
      "\n",
      "X. Li, and\n",
      "\n",
      "X. Qiu, “Tener: Adapting trans ormer encoder for name entity recognition,” arXiv preprint arXiv:1911.04474,\n",
      "\n",
      "2019. [149] Q. Wang, Y. Zhou, T. Ruan,\n",
      "\n",
      "D. Gao, Y. Xia, and P. He, “Incorporat ng dictionaries into deep neural networks for the chinese clinical named entity recognition,” J. Biomed. Inform., vol. 92,\n",
      "\n",
      "2019. [150] Y. Zhang and J. Yang, “Chinese ner using lattice lstm,” in ACL,\n",
      "\n",
      "2018, pp. 1554–1564.\n",
      "\n",
      "[151] W. Wang, F. Bao, and G. Gao, “Mongolian named entity recog ition with bidirectional recurrent neural networks,” in ICTAI,\n",
      "\n",
      "2016, pp. 495–500.\n",
      "\n",
      "[152] J. Straková,\n",
      "\n",
      "M. Straka, and J. Hajiˇc, “Neural networks for fea ureless named entity recognition in czech,” in TSD, 2016, pp.\n",
      "\n",
      "173–181.\n",
      "\n",
      "M. Gridach, “Character-aware neural networks for arabic named entity recognition for social media,” in WSSANLP, 2016, pp. 23–\n",
      "\n",
      "32. [154]\n",
      "\n",
      "M. K. Malik, “Urdu named entity recognition and classiﬁcation system using artiﬁcial neural network,” ACM Trans. Asian Low esour. Lang. Inf. Process., vol. 17, no. 1, p. 2,\n",
      "\n",
      "2017. [155] T.-H. Pham and P. Le-Hong, “End-to-end recurrent neural net ork models for vietnamese named entity recognition: Word evel vs. character-level,” in PACLING, 2017, pp. 219–232.\n",
      "\n",
      "[156] K. Kurniawan and S. Louvan, “Empirical evaluation of character ased model on neural named-entity recognition in indonesian conversational texts,” arXiv preprint arXiv:1805.12291,\n",
      "\n",
      "2018. [157] K. Yano, “Neural disease named entity extraction with character ased bilstm+ crf in japanese medical text,” arXiv preprint arXiv:1806.03648,\n",
      "\n",
      "2018. [158] A. Bharadwaj,\n",
      "\n",
      "D. Mortensen,\n",
      "\n",
      "C. Dyer, and J. Carbonell, “Phono ogically aware neural model for named entity recognition in low resource transfer settings,” in EMNLP, 2016, pp. 1462–1472.\n",
      "\n",
      "[159] J. Xie, Z. Yang, G. Neubig, N. A. Smith, and J. Carbonell, “Neural cross-lingual named entity recognition with minimal resources,” in EMNLP, 2018, pp. 369–379.\n",
      "\n",
      "[160] Y. Lin, S. Yang,\n",
      "\n",
      "V. Stoyanov, and H. Ji, “A multi-lingual multi-task architecture for low-resource sequence labeling,” in ACL, 2018, pp. 799–809.\n",
      "\n",
      "[161] R. Caruana, “Multitask learning,” Mach. learn., vol. 28, no. 1, pp.\n",
      "\n",
      "1997. [162] N. Peng and\n",
      "\n",
      "M. Dredze, “Multi-task domain adaptation for sequence tagging,” in RepL4NLP, 2017, pp. 91–100.\n",
      "\n",
      "[163] G. Crichton, S. Pyysalo, B. Chiu, and A. Korhonen, “A neural network multi-task learning approach to biomedical named en ity recognition,” BMC Bioinform., vol. 18, no. 1, p. 368,\n",
      "\n",
      "2017. [164]\n",
      "\n",
      "X. Wang, Y. Zhang,\n",
      "\n",
      "X. Ren, Y. Zhang,\n",
      "\n",
      "M. Zitnik, J. Shang,\n",
      "\n",
      "C. Langlotz, and J. Han, “Cross-type biomedical named en ity recognition with deep multi-task learning,” arXiv preprint arXiv:1801.09851,\n",
      "\n",
      "2018. [165] S. J. Pan, Q. Yang et al., “A survey on transfer learning,” IEEE\n",
      "\n",
      "Trans. Knowl. Data Eng., vol. 22, no. 10, pp. 1345–1359,\n",
      "\n",
      "2010. [166] J. Jiang and\n",
      "\n",
      "C. Zhai, “Instance weighting for domain adaptation in nlp,” in ACL, 2007, pp. 264–271.\n",
      "\n",
      "D. Wu, W. S. Lee, N. Ye, and H.\n",
      "\n",
      "L. Chieu, “Domain adaptive bootstrapping for named entity recognition,” in EMNLP, 2009, pp. 1523–1532.\n",
      "\n",
      "[168] A. Chaudhary, J. Xie, Z. Sheikh, G. Neubig, and J. G. Carbonell, low-resource named entity recognizers,” pp. 5163–5173,\n",
      "\n",
      "2019. [169] S. J. Pan, Z. Toh, and J. Su, “Transfer joint embedding for cross omain named entity recognition,” ACM Trans. Inf. Syst., vol. 31, no. 2, p. 7,\n",
      "\n",
      "2013. [170] J. Y. Lee, F. Dernoncourt, and P. Szolovits, “Transfer learning for named-entity recognition with neural networks,” arXiv preprint arXiv:1705.06273,\n",
      "\n",
      "2017. [171] B. Y. Lin and W. Lu, “Neural adaptation layers for cross-domain named entity recognition,” in EMNLP, 2018, pp. 2012–2022.\n",
      "\n",
      "[172] Y. Cao, Z. Hu, T. Chua, Z. Liu, and H. Ji, “Low-resource name tagging learned with weakly labeled data,” in EMNLP, 2019, pp.\n",
      "\n",
      "261–270.\n",
      "\n",
      "X. Huang,\n",
      "\n",
      "L. Dong, E. Boschee, and N. Peng, “Learning A uniﬁed named entity tagger from multiple partially annotated corpora for efﬁcient adaptation,” in CoNLL, 2019, pp. 515–527.\n",
      "\n",
      "L. Qu, G. Ferraro,\n",
      "\n",
      "L. Zhou, W. Hou, and T. Baldwin, “Named en ity recognition for novel types by transfer learning,” in EMNLP,\n",
      "\n",
      "2016, pp. 899–905.\n",
      "\n",
      "[175] Z. Yang, R. Salakhutdinov, and W. W. Cohen, “Transfer learning for sequence tagging with hierarchical recurrent networks,” in\n",
      "\n",
      "2017. [176] P. von Däniken and\n",
      "\n",
      "M. Cieliebak, “Transfer learning and sentence level features for named entity recognition on tweets,” in W-NUT,\n",
      "\n",
      "2017, pp. 166–171.\n",
      "\n",
      "[177] H. Zhao, Y. Yang, Q. Zhang, and\n",
      "\n",
      "L. Si, “Improve neural entity recognition via multi-task data selection and constrained decod ng,” in NAACL-HLT, vol. 2, 2018, pp. 346–351.\n",
      "\n",
      "[178] G. Beryozkin, Y. Drori, O. Gilon, T. Hartman, and\n",
      "\n",
      "I. Szpektor, “A joint named-entity recognizer for heterogeneous tag-sets using a tag hierarchy,” in ACL, 2019, pp. 140–150.\n",
      "\n",
      "[179] J.\n",
      "\n",
      "M. Giorgi and G.\n",
      "\n",
      "D. Bader, “Transfer learning for biomedical named entity recognition with neural networks,” Bioinformatics,\n",
      "\n",
      "2018. [180] Z. Wang, Y. Qu,\n",
      "\n",
      "L. Chen, J. Shen, W. Zhang, S. Zhang, Y. Gao,\n",
      "\n",
      "G. Gu, K. Chen, and Y. Yu, “Label-aware double transfer learning for cross-specialty medical named entity recognition,” in NAACL LT, 2018, pp. 1–15.\n",
      "\n",
      "[181] B. Settles, “Active learning,” Synth. Lect. Artif. Intell. Mach. Learn., vol. 6, no. 1, pp. 1–114,\n",
      "\n",
      "2012. IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, 2020\n",
      "\n",
      "D. D. Lewis and W. A. Gale, “A sequential algorithm for training text classiﬁers,” in SIGIR, 1994, pp. 3–12.\n",
      "\n",
      "[183] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Björkelund,\n",
      "\n",
      "O. Uryupina, Y. Zhang, and Z. Zhong, “Towards robust linguistic analysis using ontonotes,” in CoNLL, 2013, pp. 143–152.\n",
      "\n",
      "L. P. Kaelbling,\n",
      "\n",
      "M. L. Littman, and A. W. Moore, “Reinforcement learning: A survey,” J. Artif. Intell. Res., vol. 4, pp. 237–285,\n",
      "\n",
      "1996. [185] R. S. Sutton and A. G. Barto, Introduction to reinforcement learning. MIT press Cambridge, 1998, vol.\n",
      "\n",
      "135. [186] S.\n",
      "\n",
      "D. Sahoo, J. Lu, and P. Zhao, “Online learning: A comprehensive survey,” arXiv preprint arXiv:1802.02871,\n",
      "\n",
      "2018. [187] K. Narasimhan, A. Yala, and R. Barzilay, “Improving information extraction by acquiring external evidence with reinforcement learning,” in EMNLP, 2016, pp. 2355–2365.\n",
      "\n",
      "V. Mnih, K. Kavukcuoglu,\n",
      "\n",
      "D. Silver, A. A. Rusu, J. Veness,\n",
      "\n",
      "M. G. Bellemare, A. Graves,\n",
      "\n",
      "M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., “Human-level control through deep reinforcement learn ng,” Nature, vol. 518, no. 7540, p. 529,\n",
      "\n",
      "2015. [189] Y. Yang, W. Chen, Z. Li, Z. He, and\n",
      "\n",
      "M. Zhang, “Distantly super ised NER with partial annotation learning and reinforcement learning,” in COLING, 2018, pp. 2159–2169.\n",
      "\n",
      "D. Lowd and\n",
      "\n",
      "C. Meek, “Adversarial learning,” in SIGKDD, 2005, pp. 641–647.\n",
      "\n",
      "I. Goodfellow, J. Pouget-Abadie,\n",
      "\n",
      "M. Mirza, B. Xu,\n",
      "\n",
      "D. Warde arley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver arial nets,” in NIPS, 2014, pp. 2672–2680.\n",
      "\n",
      "L. Huang, H. Ji, and J. May, “Cross-lingual multi-level adversarial transfer to enhance low-resource name tagging,” in NAACL-HLT,\n",
      "\n",
      "2019, pp. 3823–3833.\n",
      "\n",
      "[193] J. Li,\n",
      "\n",
      "D. Ye, and S. Shang, “Adversarial transfer for named entity boundary detection with pointer networks,” in IJCAI, 2019, pp.\n",
      "\n",
      "5053–5059.\n",
      "\n",
      "[194] P. Cao, Y. Chen, K. Liu, J. Zhao, and S. Liu, “Adversarial transfer learning for chinese named entity recognition with self-attention mechanism,” in EMNLP, 2018, pp. 182–192.\n",
      "\n",
      "[195] J. T. Zhou, H. Zhang,\n",
      "\n",
      "D. Jin, H. Zhu,\n",
      "\n",
      "M. Fang, R. S. and K. Kwok, “Dual adversarial neural transfer for low-resource named entity recognition,” in ACL, 2019, pp. 3461–3471.\n",
      "\n",
      "D. Britz, “Attention and memory in deep learning and nlp,”\n",
      "\n",
      "Online: http://www. wildml. com/2016/01/attention-and-memory-in eeplearning-and-nlp,\n",
      "\n",
      "2016. [197] A. Zukov-Gregoric, Y. Bachrach, P. Minkovsky, S. Coope, and\n",
      "\n",
      "B. Maksak, “Neural named entity recognition using a self ttention mechanism,” in ICTAI, 2017, pp. 652–656.\n",
      "\n",
      "[198] G. Xu,\n",
      "\n",
      "C. Wang, and\n",
      "\n",
      "X. He, “Improving clinical named entity recognition with global neural attention,” in APWeb-WAIM, 2018, pp. 264–279.\n",
      "\n",
      "[199] Q. Zhang, J. Fu,\n",
      "\n",
      "X. Liu, and\n",
      "\n",
      "X. Huang, “Adaptive co-attention network for named entity recognition in tweets,” in AAAI,\n",
      "\n",
      "2018. [200]\n",
      "\n",
      "L. Derczynski, E. Nichols,\n",
      "\n",
      "M. van Erp, and N. Limsopatham, entity recognition,” in W-NUT, 2017, pp. 140–147.\n",
      "\n",
      "[201] J. Fisher and A. Vlachos, “Merge and label: A novel neural network architecture for nested NER,” in ACL, 2019, pp. 5840–\n",
      "\n",
      "5850. [202]\n",
      "\n",
      "D. Ye, Z. Xing,\n",
      "\n",
      "C. Y. Foo, Z. Q. Ang, J. Li, and N. Kapre, “Software peciﬁc named entity recognition in software engineering social content,” in SANER, 2016, pp. 90–101.\n",
      "\n",
      "I. Partalas,\n",
      "\n",
      "C. Lopez, N. Derbas, and R. Kalitvianski, “Learning to search for recognizing named entities in twitter,” in W-NUT,\n",
      "\n",
      "2016, pp. 171–177.\n",
      "\n",
      "[204] W. Shen, J. Han, J. Wang,\n",
      "\n",
      "X. Yuan, and Z. Yang, “Shine+: A general framework for domain-speciﬁc entity linking with het rogeneous information networks,” IEEE Trans. Knowl. Data Eng., vol. 30, no. 2, pp. 353–366,\n",
      "\n",
      "2018. [205]\n",
      "\n",
      "M. C. Phan, A. Sun, Y. Tay, J. Han, and\n",
      "\n",
      "C. Li, “Pair-linking for collective entity disambiguation: Two could be better than all,” arXiv preprint arXiv:1802.01074,\n",
      "\n",
      "2018. [206]\n",
      "\n",
      "C. Li and A. Sun, “Extracting ﬁne-grained location with temporal awareness in tweets: A two-stage approach,” J. Assoc. Inf. Sci. Technol., vol. 68, no. 7, pp. 1652–1670,\n",
      "\n",
      "2017. [207] J. Han, A. Sun, G. Cong, W.\n",
      "\n",
      "X. Zhao, Z. Ji, and\n",
      "\n",
      "M. C. Phan,\n",
      "\n",
      "2018. [208]\n",
      "\n",
      "M. C. Phan and A. Sun, “Collective named entity recognition in user comments via parameterized label propagation,” J. Assoc. Inf. Sci. Technol.,\n",
      "\n",
      "2019. [209] Z. Batmaz, A. Yurekli, A. Bilge, and\n",
      "\n",
      "C. Kaleli, “A review on deep learning for recommender systems: challenges and remedies,”\n",
      "\n",
      "Artif. Intell. Rev., pp. 1–37,\n",
      "\n",
      "2018. [210]\n",
      "\n",
      "M. Röder, R. Usbeck, and A. N. Ngomo, “GERBIL - benchmark ng named entity recognition and linking consistently,” Semantic\n",
      "\n",
      "Web, vol. 9, no. 5, pp. 605–625,\n",
      "\n",
      "2018. [211]\n",
      "\n",
      "M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng,\n",
      "\n",
      "D. Grang er, and\n",
      "\n",
      "M. Auli, “fairseq: A fast, extensible toolkit for sequence modeling,” in NAACL-HLT, 2019, pp. 48–53.\n",
      "\n",
      "[212] F. Dernoncourt, J. Y. Lee, and P. Szolovits, “NeuroNER: an easy o-use program for named-entity recognition based on neural networks,” in EMNLP, 2017, pp. 97–102.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extract('./test/pdf/05-complex.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
